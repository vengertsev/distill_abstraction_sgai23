04/24 01:56:51 PM The args: Namespace(aug_train=True, cache_dir='', data_dir='./_data/glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=6.0, output_dir='./models_train/TinyBERT_6L_768D_1122_stg2_CoLA', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_6L_768D_1122_stg1_CoLA', task_name='CoLA', teacher_model='./_models/bert-base-uncased-cola', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
04/24 01:56:51 PM device: cuda n_gpu: 1
04/24 01:56:51 PM ******** num_labels=2
04/24 01:57:03 PM Model config {
  "_name_or_path": "/mnt/lustre/weixiuying/transformer/pretrain_models/bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.13.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

04/24 01:57:03 PM Loading model ./_models/bert-base-uncased-cola/pytorch_model.bin
04/24 01:57:03 PM loading model...
04/24 01:57:04 PM done!
04/24 01:57:04 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
04/24 01:57:04 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
04/24 01:57:04 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/24 01:57:05 PM Loading model ./models_train/TinyBERT_6L_768D_1122_stg1_CoLA/pytorch_model.bin
04/24 01:57:05 PM loading model...
04/24 01:57:05 PM done!
04/24 01:57:05 PM ***** Running training *****
04/24 01:57:05 PM   Num examples = 210325
04/24 01:57:05 PM   Batch size = 32
04/24 01:57:05 PM   Num steps = 39432
04/24 01:57:05 PM n: bert.embeddings.word_embeddings.weight
04/24 01:57:05 PM n: bert.embeddings.position_embeddings.weight
04/24 01:57:05 PM n: bert.embeddings.token_type_embeddings.weight
04/24 01:57:05 PM n: bert.embeddings.LayerNorm.weight
04/24 01:57:05 PM n: bert.embeddings.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.0.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.0.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.1.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.1.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.2.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.2.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.3.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.3.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.4.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.4.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.query.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.query.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.key.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.key.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.value.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.self.value.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.attention.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.intermediate.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.intermediate.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.output.dense.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.output.dense.bias
04/24 01:57:05 PM n: bert.encoder.layer.5.output.LayerNorm.weight
04/24 01:57:05 PM n: bert.encoder.layer.5.output.LayerNorm.bias
04/24 01:57:05 PM n: bert.pooler.dense.weight
04/24 01:57:05 PM n: bert.pooler.dense.bias
04/24 01:57:05 PM n: classifier.weight
04/24 01:57:05 PM n: classifier.bias
04/24 01:57:05 PM n: fit_dense.weight
04/24 01:57:05 PM n: fit_dense.bias
04/24 01:57:05 PM Total parameters: 67547138
04/24 02:02:01 PM ***** Running evaluation *****
04/24 02:02:01 PM   Epoch = 0 iter 1999 step
04/24 02:02:01 PM   Num examples = 1043
04/24 02:02:01 PM   Batch size = 32
04/24 02:02:02 PM preds.shape (1043, 2)
04/24 02:02:02 PM ***** Eval results *****
04/24 02:02:02 PM   att_loss = 0.0
04/24 02:02:02 PM   cls_loss = 0.2407532700110579
04/24 02:02:02 PM   eval_loss = 0.4808365806485667
04/24 02:02:02 PM   global_step = 1999
04/24 02:02:02 PM   loss = 0.2407532700110579
04/24 02:02:02 PM   mcc = 0.47624469139939435
04/24 02:02:02 PM   rep_loss = 0.0
04/24 02:02:02 PM ***** Save model *****
04/24 02:06:57 PM ***** Running evaluation *****
04/24 02:06:57 PM   Epoch = 0 iter 3999 step
04/24 02:06:57 PM   Num examples = 1043
04/24 02:06:57 PM   Batch size = 32
04/24 02:06:58 PM preds.shape (1043, 2)
04/24 02:06:58 PM ***** Eval results *****
04/24 02:06:58 PM   att_loss = 0.0
04/24 02:06:58 PM   cls_loss = 0.21684790344708083
04/24 02:06:58 PM   eval_loss = 0.48526221242817963
04/24 02:06:58 PM   global_step = 3999
04/24 02:06:58 PM   loss = 0.21684790344708083
04/24 02:06:58 PM   mcc = 0.4746837140719997
04/24 02:06:58 PM   rep_loss = 0.0
04/24 02:11:54 PM ***** Running evaluation *****
04/24 02:11:54 PM   Epoch = 0 iter 5999 step
04/24 02:11:54 PM   Num examples = 1043
04/24 02:11:54 PM   Batch size = 32
04/24 02:11:55 PM preds.shape (1043, 2)
04/24 02:11:55 PM ***** Eval results *****
04/24 02:11:55 PM   att_loss = 0.0
04/24 02:11:55 PM   cls_loss = 0.2083012043962798
04/24 02:11:55 PM   eval_loss = 0.4817013018059008
04/24 02:11:55 PM   global_step = 5999
04/24 02:11:55 PM   loss = 0.2083012043962798
04/24 02:11:55 PM   mcc = 0.4867950830885312
04/24 02:11:55 PM   rep_loss = 0.0
04/24 02:11:55 PM ***** Save model *****
04/24 02:16:51 PM ***** Running evaluation *****
04/24 02:16:51 PM   Epoch = 1 iter 7999 step
04/24 02:16:51 PM   Num examples = 1043
04/24 02:16:51 PM   Batch size = 32
04/24 02:16:51 PM preds.shape (1043, 2)
04/24 02:16:51 PM ***** Eval results *****
04/24 02:16:51 PM   att_loss = 0.0
04/24 02:16:51 PM   cls_loss = 0.19464634873688264
04/24 02:16:51 PM   eval_loss = 0.4731256282239249
04/24 02:16:51 PM   global_step = 7999
04/24 02:16:51 PM   loss = 0.19464634873688264
04/24 02:16:51 PM   mcc = 0.5020753041155603
04/24 02:16:51 PM   rep_loss = 0.0
04/24 02:16:51 PM ***** Save model *****
04/24 02:21:47 PM ***** Running evaluation *****
04/24 02:21:47 PM   Epoch = 1 iter 9999 step
04/24 02:21:47 PM   Num examples = 1043
04/24 02:21:47 PM   Batch size = 32
04/24 02:21:48 PM preds.shape (1043, 2)
04/24 02:21:48 PM ***** Eval results *****
04/24 02:21:48 PM   att_loss = 0.0
04/24 02:21:48 PM   cls_loss = 0.1945577251222414
04/24 02:21:48 PM   eval_loss = 0.4661221833843173
04/24 02:21:48 PM   global_step = 9999
04/24 02:21:48 PM   loss = 0.1945577251222414
04/24 02:21:48 PM   mcc = 0.5149087048667216
04/24 02:21:48 PM   rep_loss = 0.0
04/24 02:21:48 PM ***** Save model *****
04/24 02:26:44 PM ***** Running evaluation *****
04/24 02:26:44 PM   Epoch = 1 iter 11999 step
04/24 02:26:44 PM   Num examples = 1043
04/24 02:26:44 PM   Batch size = 32
04/24 02:26:45 PM preds.shape (1043, 2)
04/24 02:26:45 PM ***** Eval results *****
04/24 02:26:45 PM   att_loss = 0.0
04/24 02:26:45 PM   cls_loss = 0.19451295134275245
04/24 02:26:45 PM   eval_loss = 0.4684574032823245
04/24 02:26:45 PM   global_step = 11999
04/24 02:26:45 PM   loss = 0.19451295134275245
04/24 02:26:45 PM   mcc = 0.5078785333305506
04/24 02:26:45 PM   rep_loss = 0.0
04/24 02:31:40 PM ***** Running evaluation *****
04/24 02:31:40 PM   Epoch = 2 iter 13999 step
04/24 02:31:40 PM   Num examples = 1043
04/24 02:31:40 PM   Batch size = 32
04/24 02:31:41 PM preds.shape (1043, 2)
04/24 02:31:41 PM ***** Eval results *****
04/24 02:31:41 PM   att_loss = 0.0
04/24 02:31:41 PM   cls_loss = 0.19471242647421988
04/24 02:31:41 PM   eval_loss = 0.4630242312947909
04/24 02:31:41 PM   global_step = 13999
04/24 02:31:41 PM   loss = 0.19471242647421988
04/24 02:31:41 PM   mcc = 0.5104930835427627
04/24 02:31:41 PM   rep_loss = 0.0
04/24 02:36:37 PM ***** Running evaluation *****
04/24 02:36:37 PM   Epoch = 2 iter 15999 step
04/24 02:36:37 PM   Num examples = 1043
04/24 02:36:37 PM   Batch size = 32
04/24 02:36:38 PM preds.shape (1043, 2)
04/24 02:36:38 PM ***** Eval results *****
04/24 02:36:38 PM   att_loss = 0.0
04/24 02:36:38 PM   cls_loss = 0.1936749253767802
04/24 02:36:38 PM   eval_loss = 0.46620580779783655
04/24 02:36:38 PM   global_step = 15999
04/24 02:36:38 PM   loss = 0.1936749253767802
04/24 02:36:38 PM   mcc = 0.5047489222406755
04/24 02:36:38 PM   rep_loss = 0.0
04/24 02:41:33 PM ***** Running evaluation *****
04/24 02:41:33 PM   Epoch = 2 iter 17999 step
04/24 02:41:33 PM   Num examples = 1043
04/24 02:41:33 PM   Batch size = 32
04/24 02:41:34 PM preds.shape (1043, 2)
04/24 02:41:34 PM ***** Eval results *****
04/24 02:41:34 PM   att_loss = 0.0
04/24 02:41:34 PM   cls_loss = 0.19367727915364116
04/24 02:41:34 PM   eval_loss = 0.46247205702644406
04/24 02:41:34 PM   global_step = 17999
04/24 02:41:34 PM   loss = 0.19367727915364116
04/24 02:41:34 PM   mcc = 0.5089318947112005
04/24 02:41:34 PM   rep_loss = 0.0
04/24 02:46:30 PM ***** Running evaluation *****
04/24 02:46:30 PM   Epoch = 3 iter 19999 step
04/24 02:46:30 PM   Num examples = 1043
04/24 02:46:30 PM   Batch size = 32
04/24 02:46:31 PM preds.shape (1043, 2)
04/24 02:46:31 PM ***** Eval results *****
04/24 02:46:31 PM   att_loss = 0.0
04/24 02:46:31 PM   cls_loss = 0.19322740015419126
04/24 02:46:31 PM   eval_loss = 0.4608325849879872
04/24 02:46:31 PM   global_step = 19999
04/24 02:46:31 PM   loss = 0.19322740015419126
04/24 02:46:31 PM   mcc = 0.509687043672971
04/24 02:46:31 PM   rep_loss = 0.0
04/24 02:51:26 PM ***** Running evaluation *****
04/24 02:51:26 PM   Epoch = 3 iter 21999 step
04/24 02:51:26 PM   Num examples = 1043
04/24 02:51:26 PM   Batch size = 32
04/24 02:51:27 PM preds.shape (1043, 2)
04/24 02:51:27 PM ***** Eval results *****
04/24 02:51:27 PM   att_loss = 0.0
04/24 02:51:27 PM   cls_loss = 0.19335720985409882
04/24 02:51:27 PM   eval_loss = 0.4598355410677014
04/24 02:51:27 PM   global_step = 21999
04/24 02:51:27 PM   loss = 0.19335720985409882
04/24 02:51:27 PM   mcc = 0.5113483885735861
04/24 02:51:27 PM   rep_loss = 0.0
04/24 02:56:23 PM ***** Running evaluation *****
04/24 02:56:23 PM   Epoch = 3 iter 23999 step
04/24 02:56:23 PM   Num examples = 1043
04/24 02:56:23 PM   Batch size = 32
04/24 02:56:24 PM preds.shape (1043, 2)
04/24 02:56:24 PM ***** Eval results *****
04/24 02:56:24 PM   att_loss = 0.0
04/24 02:56:24 PM   cls_loss = 0.19337775391756362
04/24 02:56:24 PM   eval_loss = 0.4628878093578599
04/24 02:56:24 PM   global_step = 23999
04/24 02:56:24 PM   loss = 0.19337775391756362
04/24 02:56:24 PM   mcc = 0.5020753041155603
04/24 02:56:24 PM   rep_loss = 0.0
04/24 03:01:19 PM ***** Running evaluation *****
04/24 03:01:19 PM   Epoch = 3 iter 25999 step
04/24 03:01:19 PM   Num examples = 1043
04/24 03:01:19 PM   Batch size = 32
04/24 03:01:20 PM preds.shape (1043, 2)
04/24 03:01:20 PM ***** Eval results *****
04/24 03:01:20 PM   att_loss = 0.0
04/24 03:01:20 PM   cls_loss = 0.19333084868944586
04/24 03:01:20 PM   eval_loss = 0.463212686957735
04/24 03:01:20 PM   global_step = 25999
04/24 03:01:20 PM   loss = 0.19333084868944586
04/24 03:01:20 PM   mcc = 0.5029119946692125
04/24 03:01:20 PM   rep_loss = 0.0
04/24 03:06:16 PM ***** Running evaluation *****
04/24 03:06:16 PM   Epoch = 4 iter 27999 step
04/24 03:06:16 PM   Num examples = 1043
04/24 03:06:16 PM   Batch size = 32
04/24 03:06:17 PM preds.shape (1043, 2)
04/24 03:06:17 PM ***** Eval results *****
04/24 03:06:17 PM   att_loss = 0.0
04/24 03:06:17 PM   cls_loss = 0.19295160016910434
04/24 03:06:17 PM   eval_loss = 0.46788057842940994
04/24 03:06:17 PM   global_step = 27999
04/24 03:06:17 PM   loss = 0.19295160016910434
04/24 03:06:17 PM   mcc = 0.5085739436587455
04/24 03:06:17 PM   rep_loss = 0.0
04/24 03:11:12 PM ***** Running evaluation *****
04/24 03:11:12 PM   Epoch = 4 iter 29999 step
04/24 03:11:12 PM   Num examples = 1043
04/24 03:11:12 PM   Batch size = 32
04/24 03:11:13 PM preds.shape (1043, 2)
04/24 03:11:13 PM ***** Eval results *****
04/24 03:11:13 PM   att_loss = 0.0
04/24 03:11:13 PM   cls_loss = 0.19292120807770288
04/24 03:11:13 PM   eval_loss = 0.46243885588465317
04/24 03:11:13 PM   global_step = 29999
04/24 03:11:13 PM   loss = 0.19292120807770288
04/24 03:11:13 PM   mcc = 0.5015847043424765
04/24 03:11:13 PM   rep_loss = 0.0
04/24 03:16:09 PM ***** Running evaluation *****
04/24 03:16:09 PM   Epoch = 4 iter 31999 step
04/24 03:16:09 PM   Num examples = 1043
04/24 03:16:09 PM   Batch size = 32
04/24 03:16:10 PM preds.shape (1043, 2)
04/24 03:16:10 PM ***** Eval results *****
04/24 03:16:10 PM   att_loss = 0.0
04/24 03:16:10 PM   cls_loss = 0.19290026811538113
04/24 03:16:10 PM   eval_loss = 0.4688736873142647
04/24 03:16:10 PM   global_step = 31999
04/24 03:16:10 PM   loss = 0.19290026811538113
04/24 03:16:10 PM   mcc = 0.5083718372360356
04/24 03:16:10 PM   rep_loss = 0.0
04/24 03:21:05 PM ***** Running evaluation *****
04/24 03:21:05 PM   Epoch = 5 iter 33999 step
04/24 03:21:05 PM   Num examples = 1043
04/24 03:21:05 PM   Batch size = 32
04/24 03:21:06 PM preds.shape (1043, 2)
04/24 03:21:06 PM ***** Eval results *****
04/24 03:21:06 PM   att_loss = 0.0
04/24 03:21:06 PM   cls_loss = 0.19235524337630822
04/24 03:21:06 PM   eval_loss = 0.4665944413705306
04/24 03:21:06 PM   global_step = 33999
04/24 03:21:06 PM   loss = 0.19235524337630822
04/24 03:21:06 PM   mcc = 0.5025414388334151
04/24 03:21:06 PM   rep_loss = 0.0
04/24 03:26:02 PM ***** Running evaluation *****
04/24 03:26:02 PM   Epoch = 5 iter 35999 step
04/24 03:26:02 PM   Num examples = 1043
04/24 03:26:02 PM   Batch size = 32
04/24 03:26:03 PM preds.shape (1043, 2)
04/24 03:26:03 PM ***** Eval results *****
04/24 03:26:03 PM   att_loss = 0.0
04/24 03:26:03 PM   cls_loss = 0.19233295640944675
04/24 03:26:03 PM   eval_loss = 0.4653087937922189
04/24 03:26:03 PM   global_step = 35999
04/24 03:26:03 PM   loss = 0.19233295640944675
04/24 03:26:03 PM   mcc = 0.4993820999683161
04/24 03:26:03 PM   rep_loss = 0.0
04/24 03:30:58 PM ***** Running evaluation *****
04/24 03:30:58 PM   Epoch = 5 iter 37999 step
04/24 03:30:58 PM   Num examples = 1043
04/24 03:30:58 PM   Batch size = 32
04/24 03:30:59 PM preds.shape (1043, 2)
04/24 03:30:59 PM ***** Eval results *****
04/24 03:30:59 PM   att_loss = 0.0
04/24 03:30:59 PM   cls_loss = 0.1922689893060585
04/24 03:30:59 PM   eval_loss = 0.4651484898093975
04/24 03:30:59 PM   global_step = 37999
04/24 03:30:59 PM   loss = 0.1922689893060585
04/24 03:30:59 PM   mcc = 0.5047288311874955
04/24 03:30:59 PM   rep_loss = 0.0
