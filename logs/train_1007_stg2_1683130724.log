05/03 12:18:45 PM The args: Namespace(aug_train=False, cache_dir='', data_dir='./_data/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=500, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='./models_train/TinyBERT_4L_312D_1007_stg2_SST-2', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_4L_312D_1007_stg1_SST-2', task_name='SST-2', teacher_model='./_models/bert-base-uncased-finetuned-sst2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
05/03 12:18:45 PM device: cuda n_gpu: 1
05/03 12:18:45 PM ******** num_labels=2
05/03 12:18:49 PM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "MyBertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pretrained_model_name_or_path": "bert-base-uncased",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

05/03 12:18:50 PM Loading model ./_models/bert-base-uncased-finetuned-sst2/pytorch_model.bin
05/03 12:18:50 PM loading model...
05/03 12:18:50 PM done!
05/03 12:18:50 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
05/03 12:18:50 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
05/03 12:18:51 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/03 12:18:51 PM Loading model ./models_train/TinyBERT_4L_312D_1007_stg1_SST-2/pytorch_model.bin
05/03 12:18:51 PM loading model...
05/03 12:18:51 PM done!
05/03 12:18:51 PM ***** Running training *****
05/03 12:18:51 PM   Num examples = 67349
05/03 12:18:51 PM   Batch size = 32
05/03 12:18:51 PM   Num steps = 6312
05/03 12:18:51 PM n: bert.embeddings.word_embeddings.weight
05/03 12:18:51 PM n: bert.embeddings.position_embeddings.weight
05/03 12:18:51 PM n: bert.embeddings.token_type_embeddings.weight
05/03 12:18:51 PM n: bert.embeddings.LayerNorm.weight
05/03 12:18:51 PM n: bert.embeddings.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.query.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.query.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.key.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.key.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.value.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.self.value.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.intermediate.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.intermediate.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.0.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.0.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.query.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.query.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.key.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.key.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.value.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.self.value.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.intermediate.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.intermediate.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.1.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.1.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.query.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.query.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.key.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.key.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.value.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.self.value.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.intermediate.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.intermediate.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.2.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.2.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.query.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.query.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.key.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.key.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.value.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.self.value.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.intermediate.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.intermediate.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.output.dense.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.output.dense.bias
05/03 12:18:51 PM n: bert.encoder.layer.3.output.LayerNorm.weight
05/03 12:18:51 PM n: bert.encoder.layer.3.output.LayerNorm.bias
05/03 12:18:51 PM n: bert.pooler.dense.weight
05/03 12:18:51 PM n: bert.pooler.dense.bias
05/03 12:18:51 PM n: classifier.weight
05/03 12:18:51 PM n: classifier.bias
05/03 12:18:51 PM n: fit_dense.weight
05/03 12:18:51 PM n: fit_dense.bias
05/03 12:18:51 PM Total parameters: 14591258
05/03 12:19:31 PM ***** Running evaluation *****
05/03 12:19:31 PM   Epoch = 0 iter 499 step
05/03 12:19:31 PM   Num examples = 872
05/03 12:19:31 PM   Batch size = 32
05/03 12:19:31 PM preds.shape (872, 2)
05/03 12:19:31 PM ***** Eval results *****
05/03 12:19:31 PM   acc = 0.8795871559633027
05/03 12:19:31 PM   att_loss = 0.0
05/03 12:19:31 PM   cls_loss = 0.16062142817911979
05/03 12:19:31 PM   eval_loss = 0.31031521995152744
05/03 12:19:31 PM   global_step = 499
05/03 12:19:31 PM   loss = 0.16062142817911979
05/03 12:19:31 PM   rep_loss = 0.0
05/03 12:19:31 PM ***** Save model *****
05/03 12:20:10 PM ***** Running evaluation *****
05/03 12:20:10 PM   Epoch = 0 iter 999 step
05/03 12:20:10 PM   Num examples = 872
05/03 12:20:10 PM   Batch size = 32
05/03 12:20:10 PM preds.shape (872, 2)
05/03 12:20:10 PM ***** Eval results *****
05/03 12:20:10 PM   acc = 0.8681192660550459
05/03 12:20:10 PM   att_loss = 0.0
05/03 12:20:10 PM   cls_loss = 0.1417922465069158
05/03 12:20:10 PM   eval_loss = 0.3678334674664906
05/03 12:20:10 PM   global_step = 999
05/03 12:20:10 PM   loss = 0.1417922465069158
05/03 12:20:10 PM   rep_loss = 0.0
05/03 12:20:50 PM ***** Running evaluation *****
05/03 12:20:50 PM   Epoch = 0 iter 1499 step
05/03 12:20:50 PM   Num examples = 872
05/03 12:20:50 PM   Batch size = 32
05/03 12:20:50 PM preds.shape (872, 2)
05/03 12:20:50 PM ***** Eval results *****
05/03 12:20:50 PM   acc = 0.8887614678899083
05/03 12:20:50 PM   att_loss = 0.0
05/03 12:20:50 PM   cls_loss = 0.13480501748208923
05/03 12:20:50 PM   eval_loss = 0.2996081983936684
05/03 12:20:50 PM   global_step = 1499
05/03 12:20:50 PM   loss = 0.13480501748208923
05/03 12:20:50 PM   rep_loss = 0.0
05/03 12:20:50 PM ***** Save model *****
05/03 12:21:30 PM ***** Running evaluation *****
05/03 12:21:30 PM   Epoch = 0 iter 1999 step
05/03 12:21:30 PM   Num examples = 872
05/03 12:21:30 PM   Batch size = 32
05/03 12:21:30 PM preds.shape (872, 2)
05/03 12:21:30 PM ***** Eval results *****
05/03 12:21:30 PM   acc = 0.8795871559633027
05/03 12:21:30 PM   att_loss = 0.0
05/03 12:21:30 PM   cls_loss = 0.13078181030020944
05/03 12:21:30 PM   eval_loss = 0.29712185689381193
05/03 12:21:30 PM   global_step = 1999
05/03 12:21:30 PM   loss = 0.13078181030020944
05/03 12:21:30 PM   rep_loss = 0.0
05/03 12:22:09 PM ***** Running evaluation *****
05/03 12:22:09 PM   Epoch = 1 iter 2499 step
05/03 12:22:09 PM   Num examples = 872
05/03 12:22:09 PM   Batch size = 32
05/03 12:22:09 PM preds.shape (872, 2)
05/03 12:22:09 PM ***** Eval results *****
05/03 12:22:09 PM   acc = 0.8784403669724771
05/03 12:22:09 PM   att_loss = 0.0
05/03 12:22:09 PM   cls_loss = 0.11517350366221199
05/03 12:22:09 PM   eval_loss = 0.31335452598120483
05/03 12:22:09 PM   global_step = 2499
05/03 12:22:09 PM   loss = 0.11517350366221199
05/03 12:22:09 PM   rep_loss = 0.0
05/03 12:22:49 PM ***** Running evaluation *****
05/03 12:22:49 PM   Epoch = 1 iter 2999 step
05/03 12:22:49 PM   Num examples = 872
05/03 12:22:49 PM   Batch size = 32
05/03 12:22:49 PM preds.shape (872, 2)
05/03 12:22:49 PM ***** Eval results *****
05/03 12:22:49 PM   acc = 0.8944954128440367
05/03 12:22:49 PM   att_loss = 0.0
05/03 12:22:49 PM   cls_loss = 0.11524818637018097
05/03 12:22:49 PM   eval_loss = 0.28881306626967024
05/03 12:22:49 PM   global_step = 2999
05/03 12:22:49 PM   loss = 0.11524818637018097
05/03 12:22:49 PM   rep_loss = 0.0
05/03 12:22:49 PM ***** Save model *****
05/03 12:23:28 PM ***** Running evaluation *****
05/03 12:23:28 PM   Epoch = 1 iter 3499 step
05/03 12:23:28 PM   Num examples = 872
05/03 12:23:28 PM   Batch size = 32
05/03 12:23:29 PM preds.shape (872, 2)
05/03 12:23:29 PM ***** Eval results *****
05/03 12:23:29 PM   acc = 0.8830275229357798
05/03 12:23:29 PM   att_loss = 0.0
05/03 12:23:29 PM   cls_loss = 0.11572501895995978
05/03 12:23:29 PM   eval_loss = 0.3005513389195715
05/03 12:23:29 PM   global_step = 3499
05/03 12:23:29 PM   loss = 0.11572501895995978
05/03 12:23:29 PM   rep_loss = 0.0
05/03 12:24:08 PM ***** Running evaluation *****
05/03 12:24:08 PM   Epoch = 1 iter 3999 step
05/03 12:24:08 PM   Num examples = 872
05/03 12:24:08 PM   Batch size = 32
05/03 12:24:08 PM preds.shape (872, 2)
05/03 12:24:08 PM ***** Eval results *****
05/03 12:24:08 PM   acc = 0.8841743119266054
05/03 12:24:08 PM   att_loss = 0.0
05/03 12:24:08 PM   cls_loss = 0.11585939480556977
05/03 12:24:08 PM   eval_loss = 0.3031613581946918
05/03 12:24:08 PM   global_step = 3999
05/03 12:24:08 PM   loss = 0.11585939480556977
05/03 12:24:08 PM   rep_loss = 0.0
05/03 12:24:48 PM ***** Running evaluation *****
05/03 12:24:48 PM   Epoch = 2 iter 4499 step
05/03 12:24:48 PM   Num examples = 872
05/03 12:24:48 PM   Batch size = 32
05/03 12:24:48 PM preds.shape (872, 2)
05/03 12:24:48 PM ***** Eval results *****
05/03 12:24:48 PM   acc = 0.8772935779816514
05/03 12:24:48 PM   att_loss = 0.0
05/03 12:24:48 PM   cls_loss = 0.1138267153778027
05/03 12:24:48 PM   eval_loss = 0.32096925085144384
05/03 12:24:48 PM   global_step = 4499
05/03 12:24:48 PM   loss = 0.1138267153778027
05/03 12:24:48 PM   rep_loss = 0.0
05/03 12:25:27 PM ***** Running evaluation *****
05/03 12:25:27 PM   Epoch = 2 iter 4999 step
05/03 12:25:27 PM   Num examples = 872
05/03 12:25:27 PM   Batch size = 32
05/03 12:25:27 PM preds.shape (872, 2)
05/03 12:25:27 PM ***** Eval results *****
05/03 12:25:27 PM   acc = 0.8899082568807339
05/03 12:25:27 PM   att_loss = 0.0
05/03 12:25:27 PM   cls_loss = 0.11317238006293397
05/03 12:25:27 PM   eval_loss = 0.3075082812990461
05/03 12:25:27 PM   global_step = 4999
05/03 12:25:27 PM   loss = 0.11317238006293397
05/03 12:25:27 PM   rep_loss = 0.0
05/03 12:26:07 PM ***** Running evaluation *****
05/03 12:26:07 PM   Epoch = 2 iter 5499 step
05/03 12:26:07 PM   Num examples = 872
05/03 12:26:07 PM   Batch size = 32
05/03 12:26:07 PM preds.shape (872, 2)
05/03 12:26:07 PM ***** Eval results *****
05/03 12:26:07 PM   acc = 0.8853211009174312
05/03 12:26:07 PM   att_loss = 0.0
05/03 12:26:07 PM   cls_loss = 0.11272356787871059
05/03 12:26:07 PM   eval_loss = 0.30611330536859377
05/03 12:26:07 PM   global_step = 5499
05/03 12:26:07 PM   loss = 0.11272356787871059
05/03 12:26:07 PM   rep_loss = 0.0
05/03 12:26:47 PM ***** Running evaluation *****
05/03 12:26:47 PM   Epoch = 2 iter 5999 step
05/03 12:26:47 PM   Num examples = 872
05/03 12:26:47 PM   Batch size = 32
05/03 12:26:47 PM preds.shape (872, 2)
05/03 12:26:47 PM ***** Eval results *****
05/03 12:26:47 PM   acc = 0.8876146788990825
05/03 12:26:47 PM   att_loss = 0.0
05/03 12:26:47 PM   cls_loss = 0.11261358429921665
05/03 12:26:47 PM   eval_loss = 0.30479006229766775
05/03 12:26:47 PM   global_step = 5999
05/03 12:26:47 PM   loss = 0.11261358429921665
05/03 12:26:47 PM   rep_loss = 0.0
