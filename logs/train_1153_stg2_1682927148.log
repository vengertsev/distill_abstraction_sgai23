05/01 03:45:48 AM The args: Namespace(aug_train=True, cache_dir='', data_dir='./_data/glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=4.0, output_dir='./models_train/TinyBERT_6L_768D_1153_stg2_RTE', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_6L_768D_1153_stg1_RTE', task_name='RTE', teacher_model='./_models/bert-base-uncased-rte', temperature=1.0, train_batch_size=28, warmup_proportion=0.1, weight_decay=0.0001)
05/01 03:45:48 AM device: cuda n_gpu: 1
05/01 03:45:48 AM ******** num_labels=2
05/01 03:46:33 AM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "training": "",
  "transformers_version": "4.6.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

05/01 03:46:34 AM Loading model ./_models/bert-base-uncased-rte/pytorch_model.bin
05/01 03:46:34 AM loading model...
05/01 03:46:34 AM done!
05/01 03:46:34 AM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
05/01 03:46:34 AM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
05/01 03:46:35 AM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/01 03:46:35 AM Loading model ./models_train/TinyBERT_6L_768D_1153_stg1_RTE/pytorch_model.bin
05/01 03:46:36 AM loading model...
05/01 03:46:36 AM done!
05/01 03:46:36 AM ***** Running training *****
05/01 03:46:36 AM   Num examples = 144076
05/01 03:46:36 AM   Batch size = 28
05/01 03:46:36 AM   Num steps = 20580
05/01 03:46:36 AM n: bert.embeddings.word_embeddings.weight
05/01 03:46:36 AM n: bert.embeddings.position_embeddings.weight
05/01 03:46:36 AM n: bert.embeddings.token_type_embeddings.weight
05/01 03:46:36 AM n: bert.embeddings.LayerNorm.weight
05/01 03:46:36 AM n: bert.embeddings.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.0.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.0.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.1.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.1.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.2.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.2.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.3.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.3.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.4.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.4.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.query.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.query.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.key.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.key.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.value.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.self.value.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.attention.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.intermediate.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.intermediate.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.output.dense.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.output.dense.bias
05/01 03:46:36 AM n: bert.encoder.layer.5.output.LayerNorm.weight
05/01 03:46:36 AM n: bert.encoder.layer.5.output.LayerNorm.bias
05/01 03:46:36 AM n: bert.pooler.dense.weight
05/01 03:46:36 AM n: bert.pooler.dense.bias
05/01 03:46:36 AM n: classifier.weight
05/01 03:46:36 AM n: classifier.bias
05/01 03:46:36 AM n: fit_dense.weight
05/01 03:46:36 AM n: fit_dense.bias
05/01 03:46:36 AM Total parameters: 67547138
05/01 03:50:48 AM ***** Running evaluation *****
05/01 03:50:48 AM   Epoch = 0 iter 1999 step
05/01 03:50:48 AM   Num examples = 277
05/01 03:50:48 AM   Batch size = 32
05/01 03:50:48 AM preds.shape (277, 2)
05/01 03:50:48 AM ***** Eval results *****
05/01 03:50:48 AM   acc = 0.6462093862815884
05/01 03:50:48 AM   att_loss = 0.0
05/01 03:50:48 AM   cls_loss = 0.2541349239800202
05/01 03:50:48 AM   eval_loss = 0.6450970636473762
05/01 03:50:48 AM   global_step = 1999
05/01 03:50:48 AM   loss = 0.2541349239800202
05/01 03:50:48 AM   rep_loss = 0.0
05/01 03:50:48 AM ***** Save model *****
05/01 03:55:01 AM ***** Running evaluation *****
05/01 03:55:01 AM   Epoch = 0 iter 3999 step
05/01 03:55:01 AM   Num examples = 277
05/01 03:55:01 AM   Batch size = 32
05/01 03:55:01 AM preds.shape (277, 2)
05/01 03:55:01 AM ***** Eval results *****
05/01 03:55:01 AM   acc = 0.5703971119133574
05/01 03:55:01 AM   att_loss = 0.0
05/01 03:55:01 AM   cls_loss = 0.2521428806606189
05/01 03:55:01 AM   eval_loss = 0.7030155460039774
05/01 03:55:01 AM   global_step = 3999
05/01 03:55:01 AM   loss = 0.2521428806606189
05/01 03:55:01 AM   rep_loss = 0.0
05/01 03:59:14 AM ***** Running evaluation *****
05/01 03:59:14 AM   Epoch = 1 iter 5999 step
05/01 03:59:14 AM   Num examples = 277
05/01 03:59:14 AM   Batch size = 32
05/01 03:59:14 AM preds.shape (277, 2)
05/01 03:59:14 AM ***** Eval results *****
05/01 03:59:14 AM   acc = 0.6462093862815884
05/01 03:59:14 AM   att_loss = 0.0
05/01 03:59:14 AM   cls_loss = 0.2500420572362683
05/01 03:59:14 AM   eval_loss = 0.6564850277370877
05/01 03:59:14 AM   global_step = 5999
05/01 03:59:14 AM   loss = 0.2500420572362683
05/01 03:59:14 AM   rep_loss = 0.0
05/01 04:03:27 AM ***** Running evaluation *****
05/01 04:03:27 AM   Epoch = 1 iter 7999 step
05/01 04:03:27 AM   Num examples = 277
05/01 04:03:27 AM   Batch size = 32
05/01 04:03:27 AM preds.shape (277, 2)
05/01 04:03:27 AM ***** Eval results *****
05/01 04:03:27 AM   acc = 0.6209386281588448
05/01 04:03:27 AM   att_loss = 0.0
05/01 04:03:27 AM   cls_loss = 0.24966948751957999
05/01 04:03:27 AM   eval_loss = 0.66338050365448
05/01 04:03:27 AM   global_step = 7999
05/01 04:03:27 AM   loss = 0.24966948751957999
05/01 04:03:27 AM   rep_loss = 0.0
05/01 04:07:39 AM ***** Running evaluation *****
05/01 04:07:39 AM   Epoch = 1 iter 9999 step
05/01 04:07:39 AM   Num examples = 277
05/01 04:07:39 AM   Batch size = 32
05/01 04:07:40 AM preds.shape (277, 2)
05/01 04:07:40 AM ***** Eval results *****
05/01 04:07:40 AM   acc = 0.592057761732852
05/01 04:07:40 AM   att_loss = 0.0
05/01 04:07:40 AM   cls_loss = 0.24947952463774706
05/01 04:07:40 AM   eval_loss = 0.6703670223553976
05/01 04:07:40 AM   global_step = 9999
05/01 04:07:40 AM   loss = 0.24947952463774706
05/01 04:07:40 AM   rep_loss = 0.0
05/01 04:11:52 AM ***** Running evaluation *****
05/01 04:11:52 AM   Epoch = 2 iter 11999 step
05/01 04:11:52 AM   Num examples = 277
05/01 04:11:52 AM   Batch size = 32
05/01 04:11:52 AM preds.shape (277, 2)
05/01 04:11:52 AM ***** Eval results *****
05/01 04:11:52 AM   acc = 0.6823104693140795
05/01 04:11:52 AM   att_loss = 0.0
05/01 04:11:52 AM   cls_loss = 0.24892038807944292
05/01 04:11:52 AM   eval_loss = 0.6426980826589797
05/01 04:11:52 AM   global_step = 11999
05/01 04:11:52 AM   loss = 0.24892038807944292
05/01 04:11:52 AM   rep_loss = 0.0
05/01 04:11:52 AM ***** Save model *****
05/01 04:16:05 AM ***** Running evaluation *****
05/01 04:16:05 AM   Epoch = 2 iter 13999 step
05/01 04:16:05 AM   Num examples = 277
05/01 04:16:05 AM   Batch size = 32
05/01 04:16:05 AM preds.shape (277, 2)
05/01 04:16:05 AM ***** Eval results *****
05/01 04:16:05 AM   acc = 0.6498194945848376
05/01 04:16:05 AM   att_loss = 0.0
05/01 04:16:05 AM   cls_loss = 0.24869345601002268
05/01 04:16:05 AM   eval_loss = 0.6480844351980422
05/01 04:16:05 AM   global_step = 13999
05/01 04:16:05 AM   loss = 0.24869345601002268
05/01 04:16:05 AM   rep_loss = 0.0
05/01 04:20:18 AM ***** Running evaluation *****
05/01 04:20:18 AM   Epoch = 3 iter 15999 step
05/01 04:20:18 AM   Num examples = 277
05/01 04:20:18 AM   Batch size = 32
05/01 04:20:18 AM preds.shape (277, 2)
05/01 04:20:18 AM ***** Eval results *****
05/01 04:20:18 AM   acc = 0.6245487364620939
05/01 04:20:18 AM   att_loss = 0.0
05/01 04:20:18 AM   cls_loss = 0.24898868030373086
05/01 04:20:18 AM   eval_loss = 0.649617506398095
05/01 04:20:18 AM   global_step = 15999
05/01 04:20:18 AM   loss = 0.24898868030373086
05/01 04:20:18 AM   rep_loss = 0.0
05/01 04:24:31 AM ***** Running evaluation *****
05/01 04:24:31 AM   Epoch = 3 iter 17999 step
05/01 04:24:31 AM   Num examples = 277
05/01 04:24:31 AM   Batch size = 32
05/01 04:24:31 AM preds.shape (277, 2)
05/01 04:24:31 AM ***** Eval results *****
05/01 04:24:31 AM   acc = 0.6750902527075813
05/01 04:24:31 AM   att_loss = 0.0
05/01 04:24:31 AM   cls_loss = 0.24846147182114225
05/01 04:24:31 AM   eval_loss = 0.629463447464837
05/01 04:24:31 AM   global_step = 17999
05/01 04:24:31 AM   loss = 0.24846147182114225
05/01 04:24:31 AM   rep_loss = 0.0
05/01 04:28:44 AM ***** Running evaluation *****
05/01 04:28:44 AM   Epoch = 3 iter 19999 step
05/01 04:28:44 AM   Num examples = 277
05/01 04:28:44 AM   Batch size = 32
05/01 04:28:44 AM preds.shape (277, 2)
05/01 04:28:44 AM ***** Eval results *****
05/01 04:28:44 AM   acc = 0.6534296028880866
05/01 04:28:44 AM   att_loss = 0.0
05/01 04:28:44 AM   cls_loss = 0.24849187419811014
05/01 04:28:44 AM   eval_loss = 0.6350879801644219
05/01 04:28:44 AM   global_step = 19999
05/01 04:28:44 AM   loss = 0.24849187419811014
05/01 04:28:44 AM   rep_loss = 0.0
