05/03 10:32:43 PM The args: Namespace(aug_train=False, cache_dir='', data_dir='./_data/glue_data/QNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=500, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='./models_train/TinyBERT_4L_312D_1167_stg2_QNLI', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_4L_312D_1167_stg1_QNLI', task_name='QNLI', teacher_model='./_models/bert-base-uncased-QNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
05/03 10:32:43 PM device: cuda n_gpu: 1
05/03 10:32:43 PM ******** num_labels=2
05/03 10:33:08 PM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/03 10:33:09 PM Loading model ./_models/bert-base-uncased-QNLI/pytorch_model.bin
05/03 10:33:09 PM loading model...
05/03 10:33:09 PM done!
05/03 10:33:09 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
05/03 10:33:10 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/03 10:33:10 PM Loading model ./models_train/TinyBERT_4L_312D_1167_stg1_QNLI/pytorch_model.bin
05/03 10:33:10 PM loading model...
05/03 10:33:10 PM done!
05/03 10:33:10 PM ***** Running training *****
05/03 10:33:10 PM   Num examples = 104743
05/03 10:33:10 PM   Batch size = 32
05/03 10:33:10 PM   Num steps = 9819
05/03 10:33:10 PM n: bert.embeddings.word_embeddings.weight
05/03 10:33:10 PM n: bert.embeddings.position_embeddings.weight
05/03 10:33:10 PM n: bert.embeddings.token_type_embeddings.weight
05/03 10:33:10 PM n: bert.embeddings.LayerNorm.weight
05/03 10:33:10 PM n: bert.embeddings.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.query.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.query.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.key.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.key.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.value.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.self.value.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.intermediate.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.intermediate.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.0.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.0.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.query.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.query.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.key.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.key.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.value.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.self.value.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.intermediate.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.intermediate.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.1.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.1.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.query.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.query.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.key.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.key.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.value.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.self.value.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.intermediate.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.intermediate.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.2.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.2.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.query.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.query.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.key.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.key.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.value.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.self.value.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.intermediate.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.intermediate.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.output.dense.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.output.dense.bias
05/03 10:33:10 PM n: bert.encoder.layer.3.output.LayerNorm.weight
05/03 10:33:10 PM n: bert.encoder.layer.3.output.LayerNorm.bias
05/03 10:33:10 PM n: bert.pooler.dense.weight
05/03 10:33:10 PM n: bert.pooler.dense.bias
05/03 10:33:10 PM n: classifier.weight
05/03 10:33:10 PM n: classifier.bias
05/03 10:33:10 PM n: fit_dense.weight
05/03 10:33:10 PM n: fit_dense.bias
05/03 10:33:10 PM Total parameters: 14591258
05/03 10:33:50 PM ***** Running evaluation *****
05/03 10:33:50 PM   Epoch = 0 iter 499 step
05/03 10:33:50 PM   Num examples = 5463
05/03 10:33:50 PM   Batch size = 32
05/03 10:33:51 PM preds.shape (5463, 2)
05/03 10:33:51 PM ***** Eval results *****
05/03 10:33:51 PM   acc = 0.8389163463298553
05/03 10:33:51 PM   att_loss = 0.0
05/03 10:33:51 PM   cls_loss = 0.2521218738121117
05/03 10:33:51 PM   eval_loss = 0.39382198078241965
05/03 10:33:51 PM   global_step = 499
05/03 10:33:51 PM   loss = 0.2521218738121117
05/03 10:33:51 PM   rep_loss = 0.0
05/03 10:33:51 PM ***** Save model *****
05/03 10:34:30 PM ***** Running evaluation *****
05/03 10:34:30 PM   Epoch = 0 iter 999 step
05/03 10:34:30 PM   Num examples = 5463
05/03 10:34:30 PM   Batch size = 32
05/03 10:34:31 PM preds.shape (5463, 2)
05/03 10:34:31 PM ***** Eval results *****
05/03 10:34:31 PM   acc = 0.8034047226798462
05/03 10:34:31 PM   att_loss = 0.0
05/03 10:34:31 PM   cls_loss = 0.24092925536083626
05/03 10:34:31 PM   eval_loss = 0.4412134111973277
05/03 10:34:31 PM   global_step = 999
05/03 10:34:31 PM   loss = 0.24092925536083626
05/03 10:34:31 PM   rep_loss = 0.0
05/03 10:35:11 PM ***** Running evaluation *****
05/03 10:35:11 PM   Epoch = 0 iter 1499 step
05/03 10:35:11 PM   Num examples = 5463
05/03 10:35:11 PM   Batch size = 32
05/03 10:35:12 PM preds.shape (5463, 2)
05/03 10:35:12 PM ***** Eval results *****
05/03 10:35:12 PM   acc = 0.8498993227164562
05/03 10:35:12 PM   att_loss = 0.0
05/03 10:35:12 PM   cls_loss = 0.23611846965817787
05/03 10:35:12 PM   eval_loss = 0.3646602755227284
05/03 10:35:12 PM   global_step = 1499
05/03 10:35:12 PM   loss = 0.23611846965817787
05/03 10:35:12 PM   rep_loss = 0.0
05/03 10:35:12 PM ***** Save model *****
05/03 10:35:52 PM ***** Running evaluation *****
05/03 10:35:52 PM   Epoch = 0 iter 1999 step
05/03 10:35:52 PM   Num examples = 5463
05/03 10:35:52 PM   Batch size = 32
05/03 10:35:53 PM preds.shape (5463, 2)
05/03 10:35:53 PM ***** Eval results *****
05/03 10:35:53 PM   acc = 0.8403807431814022
05/03 10:35:53 PM   att_loss = 0.0
05/03 10:35:53 PM   cls_loss = 0.23371900622548908
05/03 10:35:53 PM   eval_loss = 0.3716141409866991
05/03 10:35:53 PM   global_step = 1999
05/03 10:35:53 PM   loss = 0.23371900622548908
05/03 10:35:53 PM   rep_loss = 0.0
05/03 10:36:32 PM ***** Running evaluation *****
05/03 10:36:32 PM   Epoch = 0 iter 2499 step
05/03 10:36:32 PM   Num examples = 5463
05/03 10:36:32 PM   Batch size = 32
05/03 10:36:33 PM preds.shape (5463, 2)
05/03 10:36:33 PM ***** Eval results *****
05/03 10:36:33 PM   acc = 0.8563060589419733
05/03 10:36:33 PM   att_loss = 0.0
05/03 10:36:33 PM   cls_loss = 0.23214430776702352
05/03 10:36:33 PM   eval_loss = 0.35497773852613235
05/03 10:36:33 PM   global_step = 2499
05/03 10:36:33 PM   loss = 0.23214430776702352
05/03 10:36:33 PM   rep_loss = 0.0
05/03 10:36:33 PM ***** Save model *****
05/03 10:37:13 PM ***** Running evaluation *****
05/03 10:37:13 PM   Epoch = 0 iter 2999 step
05/03 10:37:13 PM   Num examples = 5463
05/03 10:37:13 PM   Batch size = 32
05/03 10:37:14 PM preds.shape (5463, 2)
05/03 10:37:14 PM ***** Eval results *****
05/03 10:37:14 PM   acc = 0.8372688998718653
05/03 10:37:14 PM   att_loss = 0.0
05/03 10:37:14 PM   cls_loss = 0.23079574326250146
05/03 10:37:14 PM   eval_loss = 0.37941064724796697
05/03 10:37:14 PM   global_step = 2999
05/03 10:37:14 PM   loss = 0.23079574326250146
05/03 10:37:14 PM   rep_loss = 0.0
05/03 10:37:54 PM ***** Running evaluation *****
05/03 10:37:54 PM   Epoch = 1 iter 3499 step
05/03 10:37:54 PM   Num examples = 5463
05/03 10:37:54 PM   Batch size = 32
05/03 10:37:55 PM preds.shape (5463, 2)
05/03 10:37:55 PM ***** Eval results *****
05/03 10:37:55 PM   acc = 0.8376349990847519
05/03 10:37:55 PM   att_loss = 0.0
05/03 10:37:55 PM   cls_loss = 0.2187928977397691
05/03 10:37:55 PM   eval_loss = 0.384222792864543
05/03 10:37:55 PM   global_step = 3499
05/03 10:37:55 PM   loss = 0.2187928977397691
05/03 10:37:55 PM   rep_loss = 0.0
05/03 10:38:34 PM ***** Running evaluation *****
05/03 10:38:34 PM   Epoch = 1 iter 3999 step
05/03 10:38:34 PM   Num examples = 5463
05/03 10:38:34 PM   Batch size = 32
05/03 10:38:35 PM preds.shape (5463, 2)
05/03 10:38:35 PM ***** Eval results *****
05/03 10:38:35 PM   acc = 0.841845140032949
05/03 10:38:35 PM   att_loss = 0.0
05/03 10:38:35 PM   cls_loss = 0.21756720278135017
05/03 10:38:35 PM   eval_loss = 0.38544089029058376
05/03 10:38:35 PM   global_step = 3999
05/03 10:38:35 PM   loss = 0.21756720278135017
05/03 10:38:35 PM   rep_loss = 0.0
05/03 10:39:15 PM ***** Running evaluation *****
05/03 10:39:15 PM   Epoch = 1 iter 4499 step
05/03 10:39:15 PM   Num examples = 5463
05/03 10:39:15 PM   Batch size = 32
05/03 10:39:16 PM preds.shape (5463, 2)
05/03 10:39:16 PM ***** Eval results *****
05/03 10:39:16 PM   acc = 0.8575874061870767
05/03 10:39:16 PM   att_loss = 0.0
05/03 10:39:16 PM   cls_loss = 0.21768819417980717
05/03 10:39:16 PM   eval_loss = 0.35635345738533647
05/03 10:39:16 PM   global_step = 4499
05/03 10:39:16 PM   loss = 0.21768819417980717
05/03 10:39:16 PM   rep_loss = 0.0
05/03 10:39:16 PM ***** Save model *****
05/03 10:39:55 PM ***** Running evaluation *****
05/03 10:39:55 PM   Epoch = 1 iter 4999 step
05/03 10:39:55 PM   Num examples = 5463
05/03 10:39:55 PM   Batch size = 32
05/03 10:39:56 PM preds.shape (5463, 2)
05/03 10:39:56 PM ***** Eval results *****
05/03 10:39:56 PM   acc = 0.8469705290133627
05/03 10:39:56 PM   att_loss = 0.0
05/03 10:39:56 PM   cls_loss = 0.21728830809595964
05/03 10:39:56 PM   eval_loss = 0.3700394036128507
05/03 10:39:56 PM   global_step = 4999
05/03 10:39:56 PM   loss = 0.21728830809595964
05/03 10:39:56 PM   rep_loss = 0.0
05/03 10:40:36 PM ***** Running evaluation *****
05/03 10:40:36 PM   Epoch = 1 iter 5499 step
05/03 10:40:36 PM   Num examples = 5463
05/03 10:40:36 PM   Batch size = 32
05/03 10:40:37 PM preds.shape (5463, 2)
05/03 10:40:37 PM ***** Eval results *****
05/03 10:40:37 PM   acc = 0.8427603880651656
05/03 10:40:37 PM   att_loss = 0.0
05/03 10:40:37 PM   cls_loss = 0.217182626380432
05/03 10:40:37 PM   eval_loss = 0.37575164611576595
05/03 10:40:37 PM   global_step = 5499
05/03 10:40:37 PM   loss = 0.217182626380432
05/03 10:40:37 PM   rep_loss = 0.0
05/03 10:41:17 PM ***** Running evaluation *****
05/03 10:41:17 PM   Epoch = 1 iter 5999 step
05/03 10:41:17 PM   Num examples = 5463
05/03 10:41:17 PM   Batch size = 32
05/03 10:41:18 PM preds.shape (5463, 2)
05/03 10:41:18 PM ***** Eval results *****
05/03 10:41:18 PM   acc = 0.8502654219293428
05/03 10:41:18 PM   att_loss = 0.0
05/03 10:41:18 PM   cls_loss = 0.2173215284703063
05/03 10:41:18 PM   eval_loss = 0.3577325485254589
05/03 10:41:18 PM   global_step = 5999
05/03 10:41:18 PM   loss = 0.2173215284703063
05/03 10:41:18 PM   rep_loss = 0.0
05/03 10:41:57 PM ***** Running evaluation *****
05/03 10:41:57 PM   Epoch = 1 iter 6499 step
05/03 10:41:57 PM   Num examples = 5463
05/03 10:41:57 PM   Batch size = 32
05/03 10:41:58 PM preds.shape (5463, 2)
05/03 10:41:58 PM ***** Eval results *****
05/03 10:41:58 PM   acc = 0.8369028006589786
05/03 10:41:58 PM   att_loss = 0.0
05/03 10:41:58 PM   cls_loss = 0.21691362202740277
05/03 10:41:58 PM   eval_loss = 0.3780496227636672
05/03 10:41:58 PM   global_step = 6499
05/03 10:41:58 PM   loss = 0.21691362202740277
05/03 10:41:58 PM   rep_loss = 0.0
05/03 10:42:38 PM ***** Running evaluation *****
05/03 10:42:38 PM   Epoch = 2 iter 6999 step
05/03 10:42:38 PM   Num examples = 5463
05/03 10:42:38 PM   Batch size = 32
05/03 10:42:39 PM preds.shape (5463, 2)
05/03 10:42:39 PM ***** Eval results *****
05/03 10:42:39 PM   acc = 0.8520959179937763
05/03 10:42:39 PM   att_loss = 0.0
05/03 10:42:39 PM   cls_loss = 0.21135976535595016
05/03 10:42:39 PM   eval_loss = 0.3596793796583923
05/03 10:42:39 PM   global_step = 6999
05/03 10:42:39 PM   loss = 0.21135976535595016
05/03 10:42:39 PM   rep_loss = 0.0
05/03 10:43:18 PM ***** Running evaluation *****
05/03 10:43:18 PM   Epoch = 2 iter 7499 step
05/03 10:43:18 PM   Num examples = 5463
05/03 10:43:18 PM   Batch size = 32
05/03 10:43:19 PM preds.shape (5463, 2)
05/03 10:43:19 PM ***** Eval results *****
05/03 10:43:19 PM   acc = 0.8563060589419733
05/03 10:43:19 PM   att_loss = 0.0
05/03 10:43:19 PM   cls_loss = 0.21117285520995402
05/03 10:43:19 PM   eval_loss = 0.35206813805284554
05/03 10:43:19 PM   global_step = 7499
05/03 10:43:19 PM   loss = 0.21117285520995402
05/03 10:43:19 PM   rep_loss = 0.0
05/03 10:43:59 PM ***** Running evaluation *****
05/03 10:43:59 PM   Epoch = 2 iter 7999 step
05/03 10:43:59 PM   Num examples = 5463
05/03 10:43:59 PM   Batch size = 32
05/03 10:44:00 PM preds.shape (5463, 2)
05/03 10:44:00 PM ***** Eval results *****
05/03 10:44:00 PM   acc = 0.8528281164195497
05/03 10:44:00 PM   att_loss = 0.0
05/03 10:44:00 PM   cls_loss = 0.21137262234709137
05/03 10:44:00 PM   eval_loss = 0.3627945247449373
05/03 10:44:00 PM   global_step = 7999
05/03 10:44:00 PM   loss = 0.21137262234709137
05/03 10:44:00 PM   rep_loss = 0.0
05/03 10:44:39 PM ***** Running evaluation *****
05/03 10:44:39 PM   Epoch = 2 iter 8499 step
05/03 10:44:39 PM   Num examples = 5463
05/03 10:44:39 PM   Batch size = 32
05/03 10:44:41 PM preds.shape (5463, 2)
05/03 10:44:41 PM ***** Eval results *****
05/03 10:44:41 PM   acc = 0.8427603880651656
05/03 10:44:41 PM   att_loss = 0.0
05/03 10:44:41 PM   cls_loss = 0.2108915315123625
05/03 10:44:41 PM   eval_loss = 0.3803225126531389
05/03 10:44:41 PM   global_step = 8499
05/03 10:44:41 PM   loss = 0.2108915315123625
05/03 10:44:41 PM   rep_loss = 0.0
05/03 10:45:20 PM ***** Running evaluation *****
05/03 10:45:20 PM   Epoch = 2 iter 8999 step
05/03 10:45:20 PM   Num examples = 5463
05/03 10:45:20 PM   Batch size = 32
05/03 10:45:21 PM preds.shape (5463, 2)
05/03 10:45:21 PM ***** Eval results *****
05/03 10:45:21 PM   acc = 0.8564891085484166
05/03 10:45:21 PM   att_loss = 0.0
05/03 10:45:21 PM   cls_loss = 0.21084193709161006
05/03 10:45:21 PM   eval_loss = 0.35582570844923544
05/03 10:45:21 PM   global_step = 8999
05/03 10:45:21 PM   loss = 0.21084193709161006
05/03 10:45:21 PM   rep_loss = 0.0
05/03 10:46:01 PM ***** Running evaluation *****
05/03 10:46:01 PM   Epoch = 2 iter 9499 step
05/03 10:46:01 PM   Num examples = 5463
05/03 10:46:01 PM   Batch size = 32
05/03 10:46:02 PM preds.shape (5463, 2)
05/03 10:46:02 PM ***** Eval results *****
05/03 10:46:02 PM   acc = 0.8583196046128501
05/03 10:46:02 PM   att_loss = 0.0
05/03 10:46:02 PM   cls_loss = 0.21061003992325808
05/03 10:46:02 PM   eval_loss = 0.3542437727688349
05/03 10:46:02 PM   global_step = 9499
05/03 10:46:02 PM   loss = 0.21061003992325808
05/03 10:46:02 PM   rep_loss = 0.0
05/03 10:46:02 PM ***** Save model *****
