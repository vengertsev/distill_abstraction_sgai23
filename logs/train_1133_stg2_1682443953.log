04/25 01:32:34 PM The args: Namespace(aug_train=True, cache_dir='', data_dir='./_data/glue_data/CoLA', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=4.0, output_dir='./models_train/TinyBERT_4L_312D_1133_stg2_CoLA', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_4L_312D_1133_stg1_CoLA', task_name='CoLA', teacher_model='./_models/bert-base-uncased-cola', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
04/25 01:32:34 PM device: cuda n_gpu: 1
04/25 01:32:34 PM ******** num_labels=2
04/25 01:32:46 PM Model config {
  "_name_or_path": "/mnt/lustre/weixiuying/transformer/pretrain_models/bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "unacceptable",
    "1": "acceptable"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "acceptable": 1,
    "unacceptable": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.13.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

04/25 01:32:46 PM Loading model ./_models/bert-base-uncased-cola/pytorch_model.bin
04/25 01:32:47 PM loading model...
04/25 01:32:47 PM done!
04/25 01:32:47 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
04/25 01:32:47 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
04/25 01:32:47 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/25 01:32:47 PM Loading model ./models_train/TinyBERT_4L_312D_1133_stg1_CoLA/pytorch_model.bin
04/25 01:32:47 PM loading model...
04/25 01:32:47 PM done!
04/25 01:32:47 PM ***** Running training *****
04/25 01:32:47 PM   Num examples = 210325
04/25 01:32:47 PM   Batch size = 32
04/25 01:32:47 PM   Num steps = 26288
04/25 01:32:47 PM n: bert.embeddings.word_embeddings.weight
04/25 01:32:47 PM n: bert.embeddings.position_embeddings.weight
04/25 01:32:47 PM n: bert.embeddings.token_type_embeddings.weight
04/25 01:32:47 PM n: bert.embeddings.LayerNorm.weight
04/25 01:32:47 PM n: bert.embeddings.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.query.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.query.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.key.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.key.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.value.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.self.value.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.intermediate.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.intermediate.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.0.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.0.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.query.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.query.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.key.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.key.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.value.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.self.value.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.intermediate.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.intermediate.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.1.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.1.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.query.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.query.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.key.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.key.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.value.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.self.value.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.intermediate.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.intermediate.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.2.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.2.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.query.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.query.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.key.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.key.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.value.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.self.value.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.intermediate.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.intermediate.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.output.dense.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.output.dense.bias
04/25 01:32:47 PM n: bert.encoder.layer.3.output.LayerNorm.weight
04/25 01:32:47 PM n: bert.encoder.layer.3.output.LayerNorm.bias
04/25 01:32:47 PM n: bert.pooler.dense.weight
04/25 01:32:47 PM n: bert.pooler.dense.bias
04/25 01:32:47 PM n: classifier.weight
04/25 01:32:47 PM n: classifier.bias
04/25 01:32:47 PM n: fit_dense.weight
04/25 01:32:47 PM n: fit_dense.bias
04/25 01:32:47 PM Total parameters: 14591258
04/25 01:35:25 PM ***** Running evaluation *****
04/25 01:35:25 PM   Epoch = 0 iter 1999 step
04/25 01:35:25 PM   Num examples = 1043
04/25 01:35:25 PM   Batch size = 32
04/25 01:35:25 PM preds.shape (1043, 2)
04/25 01:35:25 PM ***** Eval results *****
04/25 01:35:25 PM   att_loss = 0.0
04/25 01:35:25 PM   cls_loss = 0.2202343153858137
04/25 01:35:25 PM   eval_loss = 0.603145058407928
04/25 01:35:25 PM   global_step = 1999
04/25 01:35:25 PM   loss = 0.2202343153858137
04/25 01:35:25 PM   mcc = 0.257590557689401
04/25 01:35:25 PM   rep_loss = 0.0
04/25 01:35:25 PM ***** Save model *****
04/25 01:38:03 PM ***** Running evaluation *****
04/25 01:38:03 PM   Epoch = 0 iter 3999 step
04/25 01:38:03 PM   Num examples = 1043
04/25 01:38:03 PM   Batch size = 32
04/25 01:38:04 PM preds.shape (1043, 2)
04/25 01:38:04 PM ***** Eval results *****
04/25 01:38:04 PM   att_loss = 0.0
04/25 01:38:04 PM   cls_loss = 0.21398378701828874
04/25 01:38:04 PM   eval_loss = 0.6383217013243473
04/25 01:38:04 PM   global_step = 3999
04/25 01:38:04 PM   loss = 0.21398378701828874
04/25 01:38:04 PM   mcc = 0.2690683955716971
04/25 01:38:04 PM   rep_loss = 0.0
04/25 01:38:04 PM ***** Save model *****
04/25 01:40:41 PM ***** Running evaluation *****
04/25 01:40:41 PM   Epoch = 0 iter 5999 step
04/25 01:40:41 PM   Num examples = 1043
04/25 01:40:41 PM   Batch size = 32
04/25 01:40:42 PM preds.shape (1043, 2)
04/25 01:40:42 PM ***** Eval results *****
04/25 01:40:42 PM   att_loss = 0.0
04/25 01:40:42 PM   cls_loss = 0.21141285362204307
04/25 01:40:42 PM   eval_loss = 0.6124320563041803
04/25 01:40:42 PM   global_step = 5999
04/25 01:40:42 PM   loss = 0.21141285362204307
04/25 01:40:42 PM   mcc = 0.3083054722955785
04/25 01:40:42 PM   rep_loss = 0.0
04/25 01:40:42 PM ***** Save model *****
04/25 01:43:19 PM ***** Running evaluation *****
04/25 01:43:19 PM   Epoch = 1 iter 7999 step
04/25 01:43:19 PM   Num examples = 1043
04/25 01:43:19 PM   Batch size = 32
04/25 01:43:20 PM preds.shape (1043, 2)
04/25 01:43:20 PM ***** Eval results *****
04/25 01:43:20 PM   att_loss = 0.0
04/25 01:43:20 PM   cls_loss = 0.20724904505874028
04/25 01:43:20 PM   eval_loss = 0.6395529752427881
04/25 01:43:20 PM   global_step = 7999
04/25 01:43:20 PM   loss = 0.20724904505874028
04/25 01:43:20 PM   mcc = 0.25985611172334433
04/25 01:43:20 PM   rep_loss = 0.0
04/25 01:45:57 PM ***** Running evaluation *****
04/25 01:45:57 PM   Epoch = 1 iter 9999 step
04/25 01:45:57 PM   Num examples = 1043
04/25 01:45:57 PM   Batch size = 32
04/25 01:45:58 PM preds.shape (1043, 2)
04/25 01:45:58 PM ***** Eval results *****
04/25 01:45:58 PM   att_loss = 0.0
04/25 01:45:58 PM   cls_loss = 0.20718740333368454
04/25 01:45:58 PM   eval_loss = 0.6610026287310051
04/25 01:45:58 PM   global_step = 9999
04/25 01:45:58 PM   loss = 0.20718740333368454
04/25 01:45:58 PM   mcc = 0.26484725813957466
04/25 01:45:58 PM   rep_loss = 0.0
04/25 01:48:35 PM ***** Running evaluation *****
04/25 01:48:35 PM   Epoch = 1 iter 11999 step
04/25 01:48:35 PM   Num examples = 1043
04/25 01:48:35 PM   Batch size = 32
04/25 01:48:36 PM preds.shape (1043, 2)
04/25 01:48:36 PM ***** Eval results *****
04/25 01:48:36 PM   att_loss = 0.0
04/25 01:48:36 PM   cls_loss = 0.20700246167321307
04/25 01:48:36 PM   eval_loss = 0.6393707259134813
04/25 01:48:36 PM   global_step = 11999
04/25 01:48:36 PM   loss = 0.20700246167321307
04/25 01:48:36 PM   mcc = 0.25107336189302726
04/25 01:48:36 PM   rep_loss = 0.0
04/25 01:51:13 PM ***** Running evaluation *****
04/25 01:51:13 PM   Epoch = 2 iter 13999 step
04/25 01:51:13 PM   Num examples = 1043
04/25 01:51:13 PM   Batch size = 32
04/25 01:51:14 PM preds.shape (1043, 2)
04/25 01:51:14 PM ***** Eval results *****
04/25 01:51:14 PM   att_loss = 0.0
04/25 01:51:14 PM   cls_loss = 0.20457154411321496
04/25 01:51:14 PM   eval_loss = 0.6187590324517452
04/25 01:51:14 PM   global_step = 13999
04/25 01:51:14 PM   loss = 0.20457154411321496
04/25 01:51:14 PM   mcc = 0.29031360228665876
04/25 01:51:14 PM   rep_loss = 0.0
04/25 01:53:51 PM ***** Running evaluation *****
04/25 01:53:51 PM   Epoch = 2 iter 15999 step
04/25 01:53:51 PM   Num examples = 1043
04/25 01:53:51 PM   Batch size = 32
04/25 01:53:52 PM preds.shape (1043, 2)
04/25 01:53:52 PM ***** Eval results *****
04/25 01:53:52 PM   att_loss = 0.0
04/25 01:53:52 PM   cls_loss = 0.20427721309265195
04/25 01:53:52 PM   eval_loss = 0.6431059963775404
04/25 01:53:52 PM   global_step = 15999
04/25 01:53:52 PM   loss = 0.20427721309265195
04/25 01:53:52 PM   mcc = 0.27423353371981735
04/25 01:53:52 PM   rep_loss = 0.0
04/25 01:56:29 PM ***** Running evaluation *****
04/25 01:56:29 PM   Epoch = 2 iter 17999 step
04/25 01:56:29 PM   Num examples = 1043
04/25 01:56:29 PM   Batch size = 32
04/25 01:56:30 PM preds.shape (1043, 2)
04/25 01:56:30 PM ***** Eval results *****
04/25 01:56:30 PM   att_loss = 0.0
04/25 01:56:30 PM   cls_loss = 0.2037122068579484
04/25 01:56:30 PM   eval_loss = 0.6159676882353696
04/25 01:56:30 PM   global_step = 17999
04/25 01:56:30 PM   loss = 0.2037122068579484
04/25 01:56:30 PM   mcc = 0.3041274000628709
04/25 01:56:30 PM   rep_loss = 0.0
04/25 01:59:07 PM ***** Running evaluation *****
04/25 01:59:07 PM   Epoch = 3 iter 19999 step
04/25 01:59:07 PM   Num examples = 1043
04/25 01:59:07 PM   Batch size = 32
04/25 01:59:08 PM preds.shape (1043, 2)
04/25 01:59:08 PM ***** Eval results *****
04/25 01:59:08 PM   att_loss = 0.0
04/25 01:59:08 PM   cls_loss = 0.20189012113181948
04/25 01:59:08 PM   eval_loss = 0.6550460358460745
04/25 01:59:08 PM   global_step = 19999
04/25 01:59:08 PM   loss = 0.20189012113181948
04/25 01:59:08 PM   mcc = 0.27955249362090806
04/25 01:59:08 PM   rep_loss = 0.0
04/25 02:01:45 PM ***** Running evaluation *****
04/25 02:01:45 PM   Epoch = 3 iter 21999 step
04/25 02:01:45 PM   Num examples = 1043
04/25 02:01:45 PM   Batch size = 32
04/25 02:01:45 PM preds.shape (1043, 2)
04/25 02:01:45 PM ***** Eval results *****
04/25 02:01:45 PM   att_loss = 0.0
04/25 02:01:45 PM   cls_loss = 0.2018469899070404
04/25 02:01:45 PM   eval_loss = 0.6397167435198119
04/25 02:01:45 PM   global_step = 21999
04/25 02:01:45 PM   loss = 0.2018469899070404
04/25 02:01:45 PM   mcc = 0.2847202519143651
04/25 02:01:45 PM   rep_loss = 0.0
04/25 02:04:23 PM ***** Running evaluation *****
04/25 02:04:23 PM   Epoch = 3 iter 23999 step
04/25 02:04:23 PM   Num examples = 1043
04/25 02:04:23 PM   Batch size = 32
04/25 02:04:23 PM preds.shape (1043, 2)
04/25 02:04:23 PM ***** Eval results *****
04/25 02:04:23 PM   att_loss = 0.0
04/25 02:04:23 PM   cls_loss = 0.2013795057482567
04/25 02:04:23 PM   eval_loss = 0.6352986286986958
04/25 02:04:23 PM   global_step = 23999
04/25 02:04:23 PM   loss = 0.2013795057482567
04/25 02:04:23 PM   mcc = 0.2973072046670093
04/25 02:04:23 PM   rep_loss = 0.0
04/25 02:07:01 PM ***** Running evaluation *****
04/25 02:07:01 PM   Epoch = 3 iter 25999 step
04/25 02:07:01 PM   Num examples = 1043
04/25 02:07:01 PM   Batch size = 32
04/25 02:07:01 PM preds.shape (1043, 2)
04/25 02:07:01 PM ***** Eval results *****
04/25 02:07:01 PM   att_loss = 0.0
04/25 02:07:01 PM   cls_loss = 0.20109187921776847
04/25 02:07:01 PM   eval_loss = 0.6344886331847219
04/25 02:07:01 PM   global_step = 25999
04/25 02:07:01 PM   loss = 0.20109187921776847
04/25 02:07:01 PM   mcc = 0.3003210073570313
04/25 02:07:01 PM   rep_loss = 0.0
