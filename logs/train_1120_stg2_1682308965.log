04/24 12:02:45 AM The args: Namespace(aug_train=False, cache_dir='', data_dir='./_data/glue_data/WNLI', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=50, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='./models_train/TinyBERT_6L_768D_1120_stg2_WNLI', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_6L_768D_1120_stg1_WNLI', task_name='WNLI', teacher_model='./_models/bert-base-uncased-WNLI', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
04/24 12:02:45 AM device: cuda n_gpu: 1
04/24 12:02:45 AM ******** num_labels=2
04/24 12:02:45 AM Model config {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "glue:wnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pre_trained": "",
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/24 12:02:46 AM Loading model ./_models/bert-base-uncased-WNLI/pytorch_model.bin
04/24 12:02:46 AM loading model...
04/24 12:02:46 AM done!
04/24 12:02:46 AM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
04/24 12:02:47 AM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/24 12:02:47 AM Loading model ./models_train/TinyBERT_6L_768D_1120_stg1_WNLI/pytorch_model.bin
04/24 12:02:48 AM loading model...
04/24 12:02:48 AM done!
04/24 12:02:48 AM ***** Running training *****
04/24 12:02:48 AM   Num examples = 635
04/24 12:02:48 AM   Batch size = 32
04/24 12:02:48 AM   Num steps = 57
04/24 12:02:48 AM n: bert.embeddings.word_embeddings.weight
04/24 12:02:48 AM n: bert.embeddings.position_embeddings.weight
04/24 12:02:48 AM n: bert.embeddings.token_type_embeddings.weight
04/24 12:02:48 AM n: bert.embeddings.LayerNorm.weight
04/24 12:02:48 AM n: bert.embeddings.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.0.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.0.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.1.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.1.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.2.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.2.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.3.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.3.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.4.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.4.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.query.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.query.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.key.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.key.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.value.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.self.value.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.attention.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.intermediate.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.intermediate.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.output.dense.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.output.dense.bias
04/24 12:02:48 AM n: bert.encoder.layer.5.output.LayerNorm.weight
04/24 12:02:48 AM n: bert.encoder.layer.5.output.LayerNorm.bias
04/24 12:02:48 AM n: bert.pooler.dense.weight
04/24 12:02:48 AM n: bert.pooler.dense.bias
04/24 12:02:48 AM n: classifier.weight
04/24 12:02:48 AM n: classifier.bias
04/24 12:02:48 AM n: fit_dense.weight
04/24 12:02:48 AM n: fit_dense.bias
04/24 12:02:48 AM Total parameters: 67547138
04/24 12:02:55 AM ***** Running evaluation *****
04/24 12:02:55 AM   Epoch = 2 iter 49 step
04/24 12:02:55 AM   Num examples = 71
04/24 12:02:55 AM   Batch size = 32
04/24 12:02:55 AM preds.shape (71, 2)
04/24 12:02:55 AM ***** Eval results *****
04/24 12:02:55 AM   acc = 0.43661971830985913
04/24 12:02:55 AM   att_loss = 0.0
04/24 12:02:55 AM   cls_loss = 0.3408551541241733
04/24 12:02:55 AM   eval_loss = 0.7244972785313925
04/24 12:02:55 AM   global_step = 49
04/24 12:02:55 AM   loss = 0.3408551541241733
04/24 12:02:55 AM   rep_loss = 0.0
