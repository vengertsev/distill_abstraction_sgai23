04/24 08:04:05 PM The args: Namespace(aug_train=True, cache_dir='', data_dir='./_data/glue_data/MRPC', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=1e-05, max_seq_length=128, no_cuda=False, num_train_epochs=4.0, output_dir='./models_train/TinyBERT_4L_312D_1125_stg2_MRPC', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_4L_312D_1125_stg1_MRPC', task_name='MRPC', teacher_model='./_models/bert-base-uncased-finetuned-mrpc', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
04/24 08:04:05 PM device: cuda n_gpu: 1
04/24 08:04:05 PM ******** num_labels=2
04/24 08:05:00 PM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.15.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

04/24 08:05:00 PM Loading model ./_models/bert-base-uncased-finetuned-mrpc/pytorch_model.bin
04/24 08:05:01 PM loading model...
04/24 08:05:01 PM done!
04/24 08:05:01 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
04/24 08:05:01 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
04/24 08:05:01 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/24 08:05:01 PM Loading model ./models_train/TinyBERT_4L_312D_1125_stg1_MRPC/pytorch_model.bin
04/24 08:05:01 PM loading model...
04/24 08:05:01 PM done!
04/24 08:05:01 PM ***** Running training *****
04/24 08:05:01 PM   Num examples = 225327
04/24 08:05:01 PM   Batch size = 32
04/24 08:05:01 PM   Num steps = 28164
04/24 08:05:01 PM n: bert.embeddings.word_embeddings.weight
04/24 08:05:01 PM n: bert.embeddings.position_embeddings.weight
04/24 08:05:01 PM n: bert.embeddings.token_type_embeddings.weight
04/24 08:05:01 PM n: bert.embeddings.LayerNorm.weight
04/24 08:05:01 PM n: bert.embeddings.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.query.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.query.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.key.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.key.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.value.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.self.value.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.intermediate.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.intermediate.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.0.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.0.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.query.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.query.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.key.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.key.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.value.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.self.value.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.intermediate.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.intermediate.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.1.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.1.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.query.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.query.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.key.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.key.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.value.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.self.value.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.intermediate.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.intermediate.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.2.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.2.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.query.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.query.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.key.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.key.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.value.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.self.value.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.intermediate.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.intermediate.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.output.dense.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.output.dense.bias
04/24 08:05:01 PM n: bert.encoder.layer.3.output.LayerNorm.weight
04/24 08:05:01 PM n: bert.encoder.layer.3.output.LayerNorm.bias
04/24 08:05:01 PM n: bert.pooler.dense.weight
04/24 08:05:01 PM n: bert.pooler.dense.bias
04/24 08:05:01 PM n: classifier.weight
04/24 08:05:01 PM n: classifier.bias
04/24 08:05:01 PM n: fit_dense.weight
04/24 08:05:01 PM n: fit_dense.bias
04/24 08:05:01 PM Total parameters: 14591258
04/24 08:07:39 PM ***** Running evaluation *****
04/24 08:07:39 PM   Epoch = 0 iter 1999 step
04/24 08:07:39 PM   Num examples = 1725
04/24 08:07:39 PM   Batch size = 32
04/24 08:07:39 PM preds.shape (1725, 2)
04/24 08:07:39 PM ***** Eval results *****
04/24 08:07:39 PM   acc = 0.7327536231884058
04/24 08:07:39 PM   acc_and_f1 = 0.7782988597409357
04/24 08:07:39 PM   att_loss = 0.0
04/24 08:07:39 PM   cls_loss = 0.2961706828778836
04/24 08:07:39 PM   eval_loss = 0.5679360197650062
04/24 08:07:39 PM   f1 = 0.8238440962934658
04/24 08:07:39 PM   global_step = 1999
04/24 08:07:39 PM   loss = 0.2961706828778836
04/24 08:07:39 PM   rep_loss = 0.0
04/24 08:07:39 PM ***** Save model *****
04/24 08:10:17 PM ***** Running evaluation *****
04/24 08:10:17 PM   Epoch = 0 iter 3999 step
04/24 08:10:17 PM   Num examples = 1725
04/24 08:10:17 PM   Batch size = 32
04/24 08:10:18 PM preds.shape (1725, 2)
04/24 08:10:18 PM ***** Eval results *****
04/24 08:10:18 PM   acc = 0.735072463768116
04/24 08:10:18 PM   acc_and_f1 = 0.7799547830530882
04/24 08:10:18 PM   att_loss = 0.0
04/24 08:10:18 PM   cls_loss = 0.29157256911652896
04/24 08:10:18 PM   eval_loss = 0.5690772489265159
04/24 08:10:18 PM   f1 = 0.8248371023380605
04/24 08:10:18 PM   global_step = 3999
04/24 08:10:18 PM   loss = 0.29157256911652896
04/24 08:10:18 PM   rep_loss = 0.0
04/24 08:10:18 PM ***** Save model *****
04/24 08:12:55 PM ***** Running evaluation *****
04/24 08:12:55 PM   Epoch = 0 iter 5999 step
04/24 08:12:55 PM   Num examples = 1725
04/24 08:12:55 PM   Batch size = 32
04/24 08:12:56 PM preds.shape (1725, 2)
04/24 08:12:56 PM ***** Eval results *****
04/24 08:12:56 PM   acc = 0.7298550724637681
04/24 08:12:56 PM   acc_and_f1 = 0.7759283001738244
04/24 08:12:56 PM   att_loss = 0.0
04/24 08:12:56 PM   cls_loss = 0.28970537402367946
04/24 08:12:56 PM   eval_loss = 0.5770438120320991
04/24 08:12:56 PM   f1 = 0.8220015278838808
04/24 08:12:56 PM   global_step = 5999
04/24 08:12:56 PM   loss = 0.28970537402367946
04/24 08:12:56 PM   rep_loss = 0.0
04/24 08:15:33 PM ***** Running evaluation *****
04/24 08:15:33 PM   Epoch = 1 iter 7999 step
04/24 08:15:33 PM   Num examples = 1725
04/24 08:15:33 PM   Batch size = 32
04/24 08:15:34 PM preds.shape (1725, 2)
04/24 08:15:34 PM ***** Eval results *****
04/24 08:15:34 PM   acc = 0.7298550724637681
04/24 08:15:34 PM   acc_and_f1 = 0.776536033955708
04/24 08:15:34 PM   att_loss = 0.0
04/24 08:15:34 PM   cls_loss = 0.28562754859257344
04/24 08:15:34 PM   eval_loss = 0.5826745397514768
04/24 08:15:34 PM   f1 = 0.8232169954476479
04/24 08:15:34 PM   global_step = 7999
04/24 08:15:34 PM   loss = 0.28562754859257344
04/24 08:15:34 PM   rep_loss = 0.0
04/24 08:18:12 PM ***** Running evaluation *****
04/24 08:18:12 PM   Epoch = 1 iter 9999 step
04/24 08:18:12 PM   Num examples = 1725
04/24 08:18:12 PM   Batch size = 32
04/24 08:18:12 PM preds.shape (1725, 2)
04/24 08:18:12 PM ***** Eval results *****
04/24 08:18:12 PM   acc = 0.7356521739130435
04/24 08:18:12 PM   acc_and_f1 = 0.7797951989256338
04/24 08:18:12 PM   att_loss = 0.0
04/24 08:18:12 PM   cls_loss = 0.2853729427566071
04/24 08:18:12 PM   eval_loss = 0.5641686303748025
04/24 08:18:12 PM   f1 = 0.823938223938224
04/24 08:18:12 PM   global_step = 9999
04/24 08:18:12 PM   loss = 0.2853729427566071
04/24 08:18:12 PM   rep_loss = 0.0
04/24 08:18:12 PM ***** Save model *****
04/24 08:20:50 PM ***** Running evaluation *****
04/24 08:20:50 PM   Epoch = 1 iter 11999 step
04/24 08:20:50 PM   Num examples = 1725
04/24 08:20:50 PM   Batch size = 32
04/24 08:20:50 PM preds.shape (1725, 2)
04/24 08:20:50 PM ***** Eval results *****
04/24 08:20:50 PM   acc = 0.7391304347826086
04/24 08:20:50 PM   acc_and_f1 = 0.7818801043749364
04/24 08:20:50 PM   att_loss = 0.0
04/24 08:20:50 PM   cls_loss = 0.28546903181119326
04/24 08:20:50 PM   eval_loss = 0.5593877964549594
04/24 08:20:50 PM   f1 = 0.8246297739672642
04/24 08:20:50 PM   global_step = 11999
04/24 08:20:50 PM   loss = 0.28546903181119326
04/24 08:20:50 PM   rep_loss = 0.0
04/24 08:20:50 PM ***** Save model *****
04/24 08:23:28 PM ***** Running evaluation *****
04/24 08:23:28 PM   Epoch = 1 iter 13999 step
04/24 08:23:28 PM   Num examples = 1725
04/24 08:23:28 PM   Batch size = 32
04/24 08:23:28 PM preds.shape (1725, 2)
04/24 08:23:28 PM ***** Eval results *****
04/24 08:23:28 PM   acc = 0.7315942028985507
04/24 08:23:28 PM   acc_and_f1 = 0.7764493592647906
04/24 08:23:28 PM   att_loss = 0.0
04/24 08:23:28 PM   cls_loss = 0.28532379128512175
04/24 08:23:28 PM   eval_loss = 0.5763233375770075
04/24 08:23:28 PM   f1 = 0.8213045156310306
04/24 08:23:28 PM   global_step = 13999
04/24 08:23:28 PM   loss = 0.28532379128512175
04/24 08:23:28 PM   rep_loss = 0.0
04/24 08:26:06 PM ***** Running evaluation *****
04/24 08:26:06 PM   Epoch = 2 iter 15999 step
04/24 08:26:06 PM   Num examples = 1725
04/24 08:26:06 PM   Batch size = 32
04/24 08:26:06 PM preds.shape (1725, 2)
04/24 08:26:06 PM ***** Eval results *****
04/24 08:26:06 PM   acc = 0.7368115942028985
04/24 08:26:06 PM   acc_and_f1 = 0.7801476322491943
04/24 08:26:06 PM   att_loss = 0.0
04/24 08:26:06 PM   cls_loss = 0.2844719486228488
04/24 08:26:06 PM   eval_loss = 0.5639521269886582
04/24 08:26:06 PM   f1 = 0.82348367029549
04/24 08:26:06 PM   global_step = 15999
04/24 08:26:06 PM   loss = 0.2844719486228488
04/24 08:26:06 PM   rep_loss = 0.0
04/24 08:28:44 PM ***** Running evaluation *****
04/24 08:28:44 PM   Epoch = 2 iter 17999 step
04/24 08:28:44 PM   Num examples = 1725
04/24 08:28:44 PM   Batch size = 32
04/24 08:28:44 PM preds.shape (1725, 2)
04/24 08:28:44 PM ***** Eval results *****
04/24 08:28:44 PM   acc = 0.736231884057971
04/24 08:28:44 PM   acc_and_f1 = 0.7798349951915777
04/24 08:28:44 PM   att_loss = 0.0
04/24 08:28:44 PM   cls_loss = 0.2844372519028257
04/24 08:28:44 PM   eval_loss = 0.5660425518397931
04/24 08:28:44 PM   f1 = 0.8234381063251844
04/24 08:28:44 PM   global_step = 17999
04/24 08:28:44 PM   loss = 0.2844372519028257
04/24 08:28:44 PM   rep_loss = 0.0
04/24 08:31:22 PM ***** Running evaluation *****
04/24 08:31:22 PM   Epoch = 2 iter 19999 step
04/24 08:31:22 PM   Num examples = 1725
04/24 08:31:22 PM   Batch size = 32
04/24 08:31:22 PM preds.shape (1725, 2)
04/24 08:31:22 PM ***** Eval results *****
04/24 08:31:22 PM   acc = 0.7368115942028985
04/24 08:31:22 PM   acc_and_f1 = 0.780489453182007
04/24 08:31:22 PM   att_loss = 0.0
04/24 08:31:22 PM   cls_loss = 0.2843446543017452
04/24 08:31:22 PM   eval_loss = 0.5705858518679937
04/24 08:31:22 PM   f1 = 0.8241673121611154
04/24 08:31:22 PM   global_step = 19999
04/24 08:31:22 PM   loss = 0.2843446543017452
04/24 08:31:22 PM   rep_loss = 0.0
04/24 08:34:00 PM ***** Running evaluation *****
04/24 08:34:00 PM   Epoch = 3 iter 21999 step
04/24 08:34:00 PM   Num examples = 1725
04/24 08:34:00 PM   Batch size = 32
04/24 08:34:01 PM preds.shape (1725, 2)
04/24 08:34:01 PM ***** Eval results *****
04/24 08:34:01 PM   acc = 0.736231884057971
04/24 08:34:01 PM   acc_and_f1 = 0.7798349951915777
04/24 08:34:01 PM   att_loss = 0.0
04/24 08:34:01 PM   cls_loss = 0.2842525630236761
04/24 08:34:01 PM   eval_loss = 0.5641676452424791
04/24 08:34:01 PM   f1 = 0.8234381063251844
04/24 08:34:01 PM   global_step = 21999
04/24 08:34:01 PM   loss = 0.2842525630236761
04/24 08:34:01 PM   rep_loss = 0.0
04/24 08:36:38 PM ***** Running evaluation *****
04/24 08:36:38 PM   Epoch = 3 iter 23999 step
04/24 08:36:38 PM   Num examples = 1725
04/24 08:36:38 PM   Batch size = 32
04/24 08:36:39 PM preds.shape (1725, 2)
04/24 08:36:39 PM ***** Eval results *****
04/24 08:36:39 PM   acc = 0.7327536231884058
04/24 08:36:39 PM   acc_and_f1 = 0.778028785530577
04/24 08:36:39 PM   att_loss = 0.0
04/24 08:36:39 PM   cls_loss = 0.2839992594217392
04/24 08:36:39 PM   eval_loss = 0.5765506779706037
04/24 08:36:39 PM   f1 = 0.8233039478727482
04/24 08:36:39 PM   global_step = 23999
04/24 08:36:39 PM   loss = 0.2839992594217392
04/24 08:36:39 PM   rep_loss = 0.0
04/24 08:39:16 PM ***** Running evaluation *****
04/24 08:39:16 PM   Epoch = 3 iter 25999 step
04/24 08:39:16 PM   Num examples = 1725
04/24 08:39:16 PM   Batch size = 32
04/24 08:39:17 PM preds.shape (1725, 2)
04/24 08:39:17 PM ***** Eval results *****
04/24 08:39:17 PM   acc = 0.7356521739130435
04/24 08:39:17 PM   acc_and_f1 = 0.7795907928388748
04/24 08:39:17 PM   att_loss = 0.0
04/24 08:39:17 PM   cls_loss = 0.2839775559743767
04/24 08:39:17 PM   eval_loss = 0.5701812191141976
04/24 08:39:17 PM   f1 = 0.823529411764706
04/24 08:39:17 PM   global_step = 25999
04/24 08:39:17 PM   loss = 0.2839775559743767
04/24 08:39:17 PM   rep_loss = 0.0
04/24 08:41:54 PM ***** Running evaluation *****
04/24 08:41:54 PM   Epoch = 3 iter 27999 step
04/24 08:41:54 PM   Num examples = 1725
04/24 08:41:54 PM   Batch size = 32
04/24 08:41:55 PM preds.shape (1725, 2)
04/24 08:41:55 PM ***** Eval results *****
04/24 08:41:55 PM   acc = 0.7368115942028985
04/24 08:41:55 PM   acc_and_f1 = 0.780625441339655
04/24 08:41:55 PM   att_loss = 0.0
04/24 08:41:55 PM   cls_loss = 0.2839805856737167
04/24 08:41:55 PM   eval_loss = 0.5709034750858942
04/24 08:41:55 PM   f1 = 0.8244392884764115
04/24 08:41:55 PM   global_step = 27999
04/24 08:41:55 PM   loss = 0.2839805856737167
04/24 08:41:55 PM   rep_loss = 0.0
