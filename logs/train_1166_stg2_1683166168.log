05/03 10:09:28 PM The args: Namespace(aug_train=False, cache_dir='', data_dir='./_data/glue_data/SST-2', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=500, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=3.0, output_dir='./models_train/TinyBERT_4L_312D_1166_stg2_SST-2', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_4L_312D_1166_stg1_SST-2', task_name='SST-2', teacher_model='./_models/bert-base-uncased-finetuned-sst2', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
05/03 10:09:28 PM device: cuda n_gpu: 1
05/03 10:09:28 PM ******** num_labels=2
05/03 10:09:33 PM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "MyBertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "pretrained_model_name_or_path": "bert-base-uncased",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "training": "",
  "transformers_version": "4.9.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

05/03 10:09:34 PM Loading model ./_models/bert-base-uncased-finetuned-sst2/pytorch_model.bin
05/03 10:09:34 PM loading model...
05/03 10:09:34 PM done!
05/03 10:09:34 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
05/03 10:09:34 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
05/03 10:09:34 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/03 10:09:34 PM Loading model ./models_train/TinyBERT_4L_312D_1166_stg1_SST-2/pytorch_model.bin
05/03 10:09:34 PM loading model...
05/03 10:09:34 PM done!
05/03 10:09:34 PM ***** Running training *****
05/03 10:09:34 PM   Num examples = 67349
05/03 10:09:34 PM   Batch size = 32
05/03 10:09:34 PM   Num steps = 6312
05/03 10:09:34 PM n: bert.embeddings.word_embeddings.weight
05/03 10:09:34 PM n: bert.embeddings.position_embeddings.weight
05/03 10:09:34 PM n: bert.embeddings.token_type_embeddings.weight
05/03 10:09:34 PM n: bert.embeddings.LayerNorm.weight
05/03 10:09:34 PM n: bert.embeddings.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.query.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.query.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.key.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.key.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.value.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.self.value.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.intermediate.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.intermediate.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.0.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.0.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.query.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.query.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.key.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.key.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.value.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.self.value.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.intermediate.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.intermediate.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.1.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.1.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.query.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.query.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.key.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.key.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.value.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.self.value.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.intermediate.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.intermediate.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.2.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.2.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.query.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.query.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.key.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.key.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.value.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.self.value.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.intermediate.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.intermediate.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.output.dense.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.output.dense.bias
05/03 10:09:34 PM n: bert.encoder.layer.3.output.LayerNorm.weight
05/03 10:09:34 PM n: bert.encoder.layer.3.output.LayerNorm.bias
05/03 10:09:34 PM n: bert.pooler.dense.weight
05/03 10:09:34 PM n: bert.pooler.dense.bias
05/03 10:09:34 PM n: classifier.weight
05/03 10:09:34 PM n: classifier.bias
05/03 10:09:34 PM n: fit_dense.weight
05/03 10:09:34 PM n: fit_dense.bias
05/03 10:09:34 PM Total parameters: 14591258
05/03 10:10:14 PM ***** Running evaluation *****
05/03 10:10:14 PM   Epoch = 0 iter 499 step
05/03 10:10:14 PM   Num examples = 872
05/03 10:10:14 PM   Batch size = 32
05/03 10:10:14 PM preds.shape (872, 2)
05/03 10:10:14 PM ***** Eval results *****
05/03 10:10:14 PM   acc = 0.8795871559633027
05/03 10:10:14 PM   att_loss = 0.0
05/03 10:10:14 PM   cls_loss = 0.16062142817911979
05/03 10:10:14 PM   eval_loss = 0.31031521995152744
05/03 10:10:14 PM   global_step = 499
05/03 10:10:14 PM   loss = 0.16062142817911979
05/03 10:10:14 PM   rep_loss = 0.0
05/03 10:10:14 PM ***** Save model *****
05/03 10:10:54 PM ***** Running evaluation *****
05/03 10:10:54 PM   Epoch = 0 iter 999 step
05/03 10:10:54 PM   Num examples = 872
05/03 10:10:54 PM   Batch size = 32
05/03 10:10:54 PM preds.shape (872, 2)
05/03 10:10:54 PM ***** Eval results *****
05/03 10:10:54 PM   acc = 0.8681192660550459
05/03 10:10:54 PM   att_loss = 0.0
05/03 10:10:54 PM   cls_loss = 0.1417922465069158
05/03 10:10:54 PM   eval_loss = 0.3678334674664906
05/03 10:10:54 PM   global_step = 999
05/03 10:10:54 PM   loss = 0.1417922465069158
05/03 10:10:54 PM   rep_loss = 0.0
05/03 10:11:33 PM ***** Running evaluation *****
05/03 10:11:33 PM   Epoch = 0 iter 1499 step
05/03 10:11:33 PM   Num examples = 872
05/03 10:11:33 PM   Batch size = 32
05/03 10:11:33 PM preds.shape (872, 2)
05/03 10:11:33 PM ***** Eval results *****
05/03 10:11:33 PM   acc = 0.8887614678899083
05/03 10:11:33 PM   att_loss = 0.0
05/03 10:11:33 PM   cls_loss = 0.13480501748208923
05/03 10:11:33 PM   eval_loss = 0.2996081983936684
05/03 10:11:33 PM   global_step = 1499
05/03 10:11:33 PM   loss = 0.13480501748208923
05/03 10:11:33 PM   rep_loss = 0.0
05/03 10:11:33 PM ***** Save model *****
05/03 10:12:13 PM ***** Running evaluation *****
05/03 10:12:13 PM   Epoch = 0 iter 1999 step
05/03 10:12:13 PM   Num examples = 872
05/03 10:12:13 PM   Batch size = 32
05/03 10:12:13 PM preds.shape (872, 2)
05/03 10:12:13 PM ***** Eval results *****
05/03 10:12:13 PM   acc = 0.8795871559633027
05/03 10:12:13 PM   att_loss = 0.0
05/03 10:12:13 PM   cls_loss = 0.13078181030020944
05/03 10:12:13 PM   eval_loss = 0.29712185689381193
05/03 10:12:13 PM   global_step = 1999
05/03 10:12:13 PM   loss = 0.13078181030020944
05/03 10:12:13 PM   rep_loss = 0.0
05/03 10:12:53 PM ***** Running evaluation *****
05/03 10:12:53 PM   Epoch = 1 iter 2499 step
05/03 10:12:53 PM   Num examples = 872
05/03 10:12:53 PM   Batch size = 32
05/03 10:12:53 PM preds.shape (872, 2)
05/03 10:12:53 PM ***** Eval results *****
05/03 10:12:53 PM   acc = 0.8784403669724771
05/03 10:12:53 PM   att_loss = 0.0
05/03 10:12:53 PM   cls_loss = 0.11517350366221199
05/03 10:12:53 PM   eval_loss = 0.31335452598120483
05/03 10:12:53 PM   global_step = 2499
05/03 10:12:53 PM   loss = 0.11517350366221199
05/03 10:12:53 PM   rep_loss = 0.0
05/03 10:13:32 PM ***** Running evaluation *****
05/03 10:13:32 PM   Epoch = 1 iter 2999 step
05/03 10:13:32 PM   Num examples = 872
05/03 10:13:32 PM   Batch size = 32
05/03 10:13:32 PM preds.shape (872, 2)
05/03 10:13:32 PM ***** Eval results *****
05/03 10:13:32 PM   acc = 0.8944954128440367
05/03 10:13:32 PM   att_loss = 0.0
05/03 10:13:32 PM   cls_loss = 0.11524818637018097
05/03 10:13:32 PM   eval_loss = 0.28881306626967024
05/03 10:13:32 PM   global_step = 2999
05/03 10:13:32 PM   loss = 0.11524818637018097
05/03 10:13:32 PM   rep_loss = 0.0
05/03 10:13:32 PM ***** Save model *****
05/03 10:14:12 PM ***** Running evaluation *****
05/03 10:14:12 PM   Epoch = 1 iter 3499 step
05/03 10:14:12 PM   Num examples = 872
05/03 10:14:12 PM   Batch size = 32
05/03 10:14:12 PM preds.shape (872, 2)
05/03 10:14:12 PM ***** Eval results *****
05/03 10:14:12 PM   acc = 0.8830275229357798
05/03 10:14:12 PM   att_loss = 0.0
05/03 10:14:12 PM   cls_loss = 0.11572501895995978
05/03 10:14:12 PM   eval_loss = 0.3005513389195715
05/03 10:14:12 PM   global_step = 3499
05/03 10:14:12 PM   loss = 0.11572501895995978
05/03 10:14:12 PM   rep_loss = 0.0
05/03 10:14:51 PM ***** Running evaluation *****
05/03 10:14:51 PM   Epoch = 1 iter 3999 step
05/03 10:14:51 PM   Num examples = 872
05/03 10:14:51 PM   Batch size = 32
05/03 10:14:52 PM preds.shape (872, 2)
05/03 10:14:52 PM ***** Eval results *****
05/03 10:14:52 PM   acc = 0.8841743119266054
05/03 10:14:52 PM   att_loss = 0.0
05/03 10:14:52 PM   cls_loss = 0.11585939480556977
05/03 10:14:52 PM   eval_loss = 0.3031613581946918
05/03 10:14:52 PM   global_step = 3999
05/03 10:14:52 PM   loss = 0.11585939480556977
05/03 10:14:52 PM   rep_loss = 0.0
05/03 10:15:31 PM ***** Running evaluation *****
05/03 10:15:31 PM   Epoch = 2 iter 4499 step
05/03 10:15:31 PM   Num examples = 872
05/03 10:15:31 PM   Batch size = 32
05/03 10:15:31 PM preds.shape (872, 2)
05/03 10:15:31 PM ***** Eval results *****
05/03 10:15:31 PM   acc = 0.8772935779816514
05/03 10:15:31 PM   att_loss = 0.0
05/03 10:15:31 PM   cls_loss = 0.1138267153778027
05/03 10:15:31 PM   eval_loss = 0.32096925085144384
05/03 10:15:31 PM   global_step = 4499
05/03 10:15:31 PM   loss = 0.1138267153778027
05/03 10:15:31 PM   rep_loss = 0.0
05/03 10:16:11 PM ***** Running evaluation *****
05/03 10:16:11 PM   Epoch = 2 iter 4999 step
05/03 10:16:11 PM   Num examples = 872
05/03 10:16:11 PM   Batch size = 32
05/03 10:16:11 PM preds.shape (872, 2)
05/03 10:16:11 PM ***** Eval results *****
05/03 10:16:11 PM   acc = 0.8899082568807339
05/03 10:16:11 PM   att_loss = 0.0
05/03 10:16:11 PM   cls_loss = 0.11317238006293397
05/03 10:16:11 PM   eval_loss = 0.3075082812990461
05/03 10:16:11 PM   global_step = 4999
05/03 10:16:11 PM   loss = 0.11317238006293397
05/03 10:16:11 PM   rep_loss = 0.0
05/03 10:16:50 PM ***** Running evaluation *****
05/03 10:16:50 PM   Epoch = 2 iter 5499 step
05/03 10:16:50 PM   Num examples = 872
05/03 10:16:50 PM   Batch size = 32
05/03 10:16:50 PM preds.shape (872, 2)
05/03 10:16:50 PM ***** Eval results *****
05/03 10:16:50 PM   acc = 0.8853211009174312
05/03 10:16:50 PM   att_loss = 0.0
05/03 10:16:50 PM   cls_loss = 0.11272356787871059
05/03 10:16:50 PM   eval_loss = 0.30611330536859377
05/03 10:16:50 PM   global_step = 5499
05/03 10:16:50 PM   loss = 0.11272356787871059
05/03 10:16:50 PM   rep_loss = 0.0
05/03 10:17:30 PM ***** Running evaluation *****
05/03 10:17:30 PM   Epoch = 2 iter 5999 step
05/03 10:17:30 PM   Num examples = 872
05/03 10:17:30 PM   Batch size = 32
05/03 10:17:30 PM preds.shape (872, 2)
05/03 10:17:30 PM ***** Eval results *****
05/03 10:17:30 PM   acc = 0.8876146788990825
05/03 10:17:30 PM   att_loss = 0.0
05/03 10:17:30 PM   cls_loss = 0.11261358429921665
05/03 10:17:30 PM   eval_loss = 0.30479006229766775
05/03 10:17:30 PM   global_step = 5999
05/03 10:17:30 PM   loss = 0.11261358429921665
05/03 10:17:30 PM   rep_loss = 0.0
