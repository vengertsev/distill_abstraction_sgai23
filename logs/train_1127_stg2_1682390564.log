04/24 10:42:45 PM The args: Namespace(aug_train=True, cache_dir='', data_dir='./_data/glue_data/RTE', data_url='', do_eval=False, do_lower_case=True, eval_batch_size=32, eval_step=2000, gradient_accumulation_steps=1, learning_rate=3e-05, max_seq_length=128, no_cuda=False, num_train_epochs=5.0, output_dir='./models_train/TinyBERT_6L_768D_1127_stg2_RTE', pred_distill=True, seed=42, student_model='./models_train/TinyBERT_6L_768D_1127_stg1_RTE', task_name='RTE', teacher_model='./_models/bert-base-uncased-rte', temperature=1.0, train_batch_size=32, warmup_proportion=0.1, weight_decay=0.0001)
04/24 10:42:45 PM device: cuda n_gpu: 1
04/24 10:42:45 PM ******** num_labels=2
04/24 10:43:30 PM Model config {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "problem_type": "single_label_classification",
  "training": "",
  "transformers_version": "4.6.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

04/24 10:43:31 PM Loading model ./_models/bert-base-uncased-rte/pytorch_model.bin
04/24 10:43:31 PM loading model...
04/24 10:43:31 PM done!
04/24 10:43:31 PM Weights of TinyBertForSequenceClassification not initialized from pretrained model: ['fit_dense.weight', 'fit_dense.bias']
04/24 10:43:31 PM Weights from pretrained model not used in TinyBertForSequenceClassification: ['bert.embeddings.position_ids']
04/24 10:43:32 PM Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pre_trained": "",
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/24 10:43:32 PM Loading model ./models_train/TinyBERT_6L_768D_1127_stg1_RTE/pytorch_model.bin
04/24 10:43:32 PM loading model...
04/24 10:43:32 PM done!
04/24 10:43:32 PM ***** Running training *****
04/24 10:43:32 PM   Num examples = 144076
04/24 10:43:32 PM   Batch size = 32
04/24 10:43:32 PM   Num steps = 22510
04/24 10:43:32 PM n: bert.embeddings.word_embeddings.weight
04/24 10:43:32 PM n: bert.embeddings.position_embeddings.weight
04/24 10:43:32 PM n: bert.embeddings.token_type_embeddings.weight
04/24 10:43:32 PM n: bert.embeddings.LayerNorm.weight
04/24 10:43:32 PM n: bert.embeddings.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.0.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.0.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.1.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.1.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.2.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.2.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.3.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.3.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.4.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.4.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.query.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.query.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.key.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.key.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.value.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.self.value.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.attention.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.intermediate.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.intermediate.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.output.dense.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.output.dense.bias
04/24 10:43:32 PM n: bert.encoder.layer.5.output.LayerNorm.weight
04/24 10:43:32 PM n: bert.encoder.layer.5.output.LayerNorm.bias
04/24 10:43:32 PM n: bert.pooler.dense.weight
04/24 10:43:32 PM n: bert.pooler.dense.bias
04/24 10:43:32 PM n: classifier.weight
04/24 10:43:32 PM n: classifier.bias
04/24 10:43:32 PM n: fit_dense.weight
04/24 10:43:32 PM n: fit_dense.bias
04/24 10:43:32 PM Total parameters: 67547138
04/24 10:48:29 PM ***** Running evaluation *****
04/24 10:48:29 PM   Epoch = 0 iter 1999 step
04/24 10:48:29 PM   Num examples = 277
04/24 10:48:29 PM   Batch size = 32
04/24 10:48:29 PM preds.shape (277, 2)
04/24 10:48:29 PM ***** Eval results *****
04/24 10:48:29 PM   acc = 0.6137184115523465
04/24 10:48:29 PM   att_loss = 0.0
04/24 10:48:29 PM   cls_loss = 0.25431898298890904
04/24 10:48:29 PM   eval_loss = 0.6615337464544508
04/24 10:48:29 PM   global_step = 1999
04/24 10:48:29 PM   loss = 0.25431898298890904
04/24 10:48:29 PM   rep_loss = 0.0
04/24 10:48:29 PM ***** Save model *****
04/24 10:53:26 PM ***** Running evaluation *****
04/24 10:53:26 PM   Epoch = 0 iter 3999 step
04/24 10:53:26 PM   Num examples = 277
04/24 10:53:26 PM   Batch size = 32
04/24 10:53:27 PM preds.shape (277, 2)
04/24 10:53:27 PM ***** Eval results *****
04/24 10:53:27 PM   acc = 0.6425992779783394
04/24 10:53:27 PM   att_loss = 0.0
04/24 10:53:27 PM   cls_loss = 0.25230867969196835
04/24 10:53:27 PM   eval_loss = 0.6498932308620877
04/24 10:53:27 PM   global_step = 3999
04/24 10:53:27 PM   loss = 0.25230867969196835
04/24 10:53:27 PM   rep_loss = 0.0
04/24 10:53:27 PM ***** Save model *****
04/24 10:58:24 PM ***** Running evaluation *****
04/24 10:58:24 PM   Epoch = 1 iter 5999 step
04/24 10:58:24 PM   Num examples = 277
04/24 10:58:24 PM   Batch size = 32
04/24 10:58:24 PM preds.shape (277, 2)
04/24 10:58:24 PM ***** Eval results *****
04/24 10:58:24 PM   acc = 0.6137184115523465
04/24 10:58:24 PM   att_loss = 0.0
04/24 10:58:24 PM   cls_loss = 0.250008678358638
04/24 10:58:24 PM   eval_loss = 0.657455325126648
04/24 10:58:24 PM   global_step = 5999
04/24 10:58:24 PM   loss = 0.250008678358638
04/24 10:58:24 PM   rep_loss = 0.0
04/24 11:03:21 PM ***** Running evaluation *****
04/24 11:03:21 PM   Epoch = 1 iter 7999 step
04/24 11:03:21 PM   Num examples = 277
04/24 11:03:21 PM   Batch size = 32
04/24 11:03:21 PM preds.shape (277, 2)
04/24 11:03:21 PM ***** Eval results *****
04/24 11:03:21 PM   acc = 0.6245487364620939
04/24 11:03:21 PM   att_loss = 0.0
04/24 11:03:21 PM   cls_loss = 0.24976043834374706
04/24 11:03:21 PM   eval_loss = 0.6556426419152154
04/24 11:03:21 PM   global_step = 7999
04/24 11:03:21 PM   loss = 0.24976043834374706
04/24 11:03:21 PM   rep_loss = 0.0
04/24 11:08:18 PM ***** Running evaluation *****
04/24 11:08:18 PM   Epoch = 2 iter 9999 step
04/24 11:08:18 PM   Num examples = 277
04/24 11:08:18 PM   Batch size = 32
04/24 11:08:19 PM preds.shape (277, 2)
04/24 11:08:19 PM ***** Eval results *****
04/24 11:08:19 PM   acc = 0.6389891696750902
04/24 11:08:19 PM   att_loss = 0.0
04/24 11:08:19 PM   cls_loss = 0.24898813124278082
04/24 11:08:19 PM   eval_loss = 0.6445664299858941
04/24 11:08:19 PM   global_step = 9999
04/24 11:08:19 PM   loss = 0.24898813124278082
04/24 11:08:19 PM   rep_loss = 0.0
04/24 11:13:16 PM ***** Running evaluation *****
04/24 11:13:16 PM   Epoch = 2 iter 11999 step
04/24 11:13:16 PM   Num examples = 277
04/24 11:13:16 PM   Batch size = 32
04/24 11:13:16 PM preds.shape (277, 2)
04/24 11:13:16 PM ***** Eval results *****
04/24 11:13:16 PM   acc = 0.6642599277978339
04/24 11:13:16 PM   att_loss = 0.0
04/24 11:13:16 PM   cls_loss = 0.24879891819667338
04/24 11:13:16 PM   eval_loss = 0.6319583389494154
04/24 11:13:16 PM   global_step = 11999
04/24 11:13:16 PM   loss = 0.24879891819667338
04/24 11:13:16 PM   rep_loss = 0.0
04/24 11:13:16 PM ***** Save model *****
04/24 11:18:13 PM ***** Running evaluation *****
04/24 11:18:13 PM   Epoch = 3 iter 13999 step
04/24 11:18:13 PM   Num examples = 277
04/24 11:18:13 PM   Batch size = 32
04/24 11:18:13 PM preds.shape (277, 2)
04/24 11:18:13 PM ***** Eval results *****
04/24 11:18:13 PM   acc = 0.6823104693140795
04/24 11:18:13 PM   att_loss = 0.0
04/24 11:18:13 PM   cls_loss = 0.24793659984823657
04/24 11:18:13 PM   eval_loss = 0.6262310345967611
04/24 11:18:13 PM   global_step = 13999
04/24 11:18:13 PM   loss = 0.24793659984823657
04/24 11:18:13 PM   rep_loss = 0.0
04/24 11:18:13 PM ***** Save model *****
04/24 11:23:10 PM ***** Running evaluation *****
04/24 11:23:10 PM   Epoch = 3 iter 15999 step
04/24 11:23:10 PM   Num examples = 277
04/24 11:23:10 PM   Batch size = 32
04/24 11:23:11 PM preds.shape (277, 2)
04/24 11:23:11 PM ***** Eval results *****
04/24 11:23:11 PM   acc = 0.6425992779783394
04/24 11:23:11 PM   att_loss = 0.0
04/24 11:23:11 PM   cls_loss = 0.24858099149261378
04/24 11:23:11 PM   eval_loss = 0.64835524559021
04/24 11:23:11 PM   global_step = 15999
04/24 11:23:11 PM   loss = 0.24858099149261378
04/24 11:23:11 PM   rep_loss = 0.0
04/24 11:28:08 PM ***** Running evaluation *****
04/24 11:28:08 PM   Epoch = 3 iter 17999 step
04/24 11:28:08 PM   Num examples = 277
04/24 11:28:08 PM   Batch size = 32
04/24 11:28:08 PM preds.shape (277, 2)
04/24 11:28:08 PM ***** Eval results *****
04/24 11:28:08 PM   acc = 0.6678700361010831
04/24 11:28:08 PM   att_loss = 0.0
04/24 11:28:08 PM   cls_loss = 0.24858760728937943
04/24 11:28:08 PM   eval_loss = 0.6322613888316684
04/24 11:28:08 PM   global_step = 17999
04/24 11:28:08 PM   loss = 0.24858760728937943
04/24 11:28:08 PM   rep_loss = 0.0
04/24 11:33:05 PM ***** Running evaluation *****
04/24 11:33:05 PM   Epoch = 4 iter 19999 step
04/24 11:33:05 PM   Num examples = 277
04/24 11:33:05 PM   Batch size = 32
04/24 11:33:05 PM preds.shape (277, 2)
04/24 11:33:05 PM ***** Eval results *****
04/24 11:33:05 PM   acc = 0.6425992779783394
04/24 11:33:05 PM   att_loss = 0.0
04/24 11:33:05 PM   cls_loss = 0.24851290655609112
04/24 11:33:05 PM   eval_loss = 0.6381471554438273
04/24 11:33:05 PM   global_step = 19999
04/24 11:33:05 PM   loss = 0.24851290655609112
04/24 11:33:05 PM   rep_loss = 0.0
04/24 11:38:02 PM ***** Running evaluation *****
04/24 11:38:02 PM   Epoch = 4 iter 21999 step
04/24 11:38:02 PM   Num examples = 277
04/24 11:38:02 PM   Batch size = 32
04/24 11:38:02 PM preds.shape (277, 2)
04/24 11:38:02 PM ***** Eval results *****
04/24 11:38:02 PM   acc = 0.6534296028880866
04/24 11:38:02 PM   att_loss = 0.0
04/24 11:38:02 PM   cls_loss = 0.24825692182046016
04/24 11:38:02 PM   eval_loss = 0.6373558441797892
04/24 11:38:02 PM   global_step = 21999
04/24 11:38:02 PM   loss = 0.24825692182046016
04/24 11:38:02 PM   rep_loss = 0.0
